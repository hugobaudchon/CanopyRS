{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"CanopyRS","text":"<p>A pipeline for processing high-resolution geospatial orthomosaics to detect, segment, and classify trees across various forest biomes.</p> <p></p>"},{"location":"#what-is-canopyrs","title":"What is CanopyRS?","text":"<p>CanopyRS takes high-resolution aerial imagery and runs it through a modular component pipeline to produce per-tree detections, segmentations, and classifications. It supports state-of-the-art model architectures spanning both CNNs (Faster R-CNN, Mask R-CNN, RetinaNet) and transformers (DINO, Mask2Former, SAM 2, SAM 3). The pipeline is configurable via YAML, and ships with pre-trained models and preset configurations for common use cases.</p>"},{"location":"#how-it-works","title":"How it works","text":"<p>A CanopyRS pipeline is a sequence of components, each responsible for one step:</p> <ol> <li>Tilerizer \u2014 splits a large orthomosaic into overlapping tiles</li> <li>Detector \u2014 runs object detection on each tile</li> <li>Segmenter \u2014 runs object segmentation on each tile or refines detections into instance segmentation masks (SAM models)</li> <li>Aggregator \u2014 merges overlapping detections across tiles using NMS to obtain raster-level predictions</li> <li>Classifier \u2014 classifies each detected tree</li> </ol> <p>The pipeline handles all I/O, state management, and background tasks. Components only implement their core logic.</p>"},{"location":"#quick-links","title":"Quick links","text":""},{"location":"#getting-started","title":"Getting started","text":"<ul> <li>Installation \u2014 get CanopyRS running</li> <li>Quickstart \u2014 run inference in minutes</li> </ul>"},{"location":"#user-guide","title":"User guide","text":"<ul> <li>Components \u2014 understand each pipeline stage</li> <li>Configuration \u2014 configure pipelines via YAML</li> <li>Presets \u2014 pre-built configurations for common scenarios</li> <li>Data \u2014 download datasets for training and benchmarking</li> <li>Evaluation \u2014 NMS parameter search and benchmarking</li> <li>Training \u2014 train your own detector models</li> </ul>"},{"location":"#api-reference","title":"API reference","text":"<ul> <li>Pipeline \u2014 pipeline orchestration</li> <li>Components \u2014 component classes</li> <li>DataState \u2014 state management</li> </ul>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome. Here is how to get started.</p>"},{"location":"contributing/#development-setup","title":"Development setup","text":"<p>Follow the installation guide, then install docs dependencies:</p> <pre><code>pip install -e \".[docs]\"\n</code></pre>"},{"location":"contributing/#running-tests","title":"Running tests","text":"<p>Run all fast (unit) tests:</p> <pre><code>pytest tests/ -m \"not slow\"\n</code></pre> <p>Run everything including slow integration tests (requires the test raster under <code>assets/</code> and model weights):</p> <pre><code>pytest tests/\n</code></pre> <p>Run a specific test file:</p> <pre><code>pytest tests/engine/test_pipeline.py\n</code></pre>"},{"location":"contributing/#building-the-docs-locally","title":"Building the docs locally","text":"<pre><code>pip install -e \".[docs]\"\nmkdocs serve\n</code></pre> <p>Then open http://localhost:8000 in your browser.</p>"},{"location":"contributing/#adding-a-new-component","title":"Adding a new component","text":"<ol> <li>Create a config parser in <code>canopyrs/engine/config_parsers/mycomponent.py</code> (subclass <code>BaseConfig</code>), then add it to <code>canopyrs/engine/config_parsers/__init__.py</code> so it can be imported from the package</li> <li>Create <code>canopyrs/engine/components/mycomponent.py</code></li> <li>Subclass <code>BaseComponent</code>, declare <code>requires_state</code>, <code>requires_columns</code>, <code>produces_state</code>, <code>produces_columns</code></li> <li>Implement <code>__call__</code> with the <code>@validate_requirements</code> decorator</li> <li>Register it in <code>Pipeline.from_config()</code> in <code>pipeline.py</code></li> <li>Add a docs entry in <code>docs/user-guide/components.md</code> and a new page under <code>docs/api/components/</code> (then register it in <code>mkdocs.yml</code> nav)</li> </ol>"},{"location":"contributing/#code-style","title":"Code style","text":"<ul> <li>Follow existing patterns in the component files</li> <li>Keep component logic focused \u2014 I/O and state updates are the pipeline's job</li> <li>Add hints to <code>state_hints</code> and <code>column_hints</code> for helpful error messages</li> </ul>"},{"location":"api/data-state/","title":"DataState","text":"Source code in <code>canopyrs/engine/data_state.py</code> <pre><code>@dataclass\nclass DataState:\n    imagery_path: str = None\n    parent_output_path: str = None\n    product_name: str = None  # Derived from imagery filename or \"tiled_input\" if only tiles\n\n    tiles_path: str = None\n\n    infer_coco_path: str = None\n    infer_gdf: gpd.GeoDataFrame = None\n    infer_gdf_columns_to_pass: set = field(default_factory=set)\n    infer_gdf_columns_to_delete_on_save: List = field(default_factory=list)\n\n    background_executor: Optional = None\n    side_processes: List = field(default_factory=list)\n\n    component_output_folders: Dict = field(default_factory=dict)\n    component_output_files: Dict = field(default_factory=dict)\n\n    def update_infer_gdf(self, infer_gdf: gpd.GeoDataFrame) -&gt; None:\n        assert isinstance(infer_gdf, gpd.GeoDataFrame)\n        assert object_id_column_name in infer_gdf.columns, f\"Columns of the infer_gdf must contain a '{object_id_column_name}'.\"\n        self.infer_gdf = infer_gdf\n\n    def register_component_folder(self, component_name: str, component_id: int, folder_path: Path) -&gt; None:\n        \"\"\"\n        Register the output folder for a component.\n        \"\"\"\n        key = get_component_folder_name(component_id, component_name)\n        self.component_output_folders[key] = folder_path\n\n    def register_output_file(self, component_name: str, component_id: int, file_type: str, file_path: Path) -&gt; None:\n        \"\"\"\n        Register an output file created by a component.\n        \"\"\"\n        key = get_component_folder_name(component_id, component_name)\n\n        if key not in self.component_output_files:\n            self.component_output_files[key] = {}\n\n        self.component_output_files[key][file_type] = file_path\n\n    def get_component_folder(self, component_name: str, component_id: int) -&gt; Optional[Path]:\n        \"\"\"Get the output folder for a specific component.\"\"\"\n        key = get_component_folder_name(component_id, component_name)\n        return self.component_output_folders.get(key)\n\n    def get_output_file(self, component_name: str, component_id: int, file_type: str) -&gt; Optional[Path]:\n        \"\"\"Get a specific output file from a component.\"\"\"\n        key = get_component_folder_name(component_id, component_name)\n        if key in self.component_output_files:\n            return self.component_output_files[key].get(file_type)\n        return None\n\n    def get_latest_output_by_type(self, file_type: str) -&gt; Optional[Path]:\n        \"\"\"Get the most recent output file of a specific type from any component.\"\"\"\n        latest_id = -1\n        latest_path = None\n\n        for key, files in self.component_output_files.items():\n            if file_type in files:\n                component_id = int(key.split('_')[0])\n                if component_id &gt; latest_id:\n                    latest_id = component_id\n                    latest_path = files[file_type]\n\n        return latest_path\n\n    def get_all_outputs(self) -&gt; Dict:\n        \"\"\"Get all registered output files organized by component.\"\"\"\n        return self.component_output_files\n\n    def clean_side_processes(self, key: str = None):\n        \"\"\"\n        Wait for and process side processes.\n\n        Args:\n            key: If provided, only process side processes for this attribute.\n                 If None, process all side processes.\n        \"\"\"\n        to_process = []\n        to_keep = []\n\n        for side_process in self.side_processes:\n            if key is None or (isinstance(side_process, tuple) and side_process[0] == key):\n                to_process.append(side_process)\n            else:\n                to_keep.append(side_process)\n\n        self.side_processes = to_keep\n\n        for side_process in to_process:\n            if isinstance(side_process, tuple):\n                attribute_name = side_process[0]\n                future_or_result = side_process[1]\n\n                # Check if this is a Future object with a .result() method\n                if hasattr(future_or_result, 'result'):\n                    result = future_or_result.result()\n                else:\n                    result = future_or_result  # It's already a result\n\n                # Update the data_state attribute\n                if attribute_name:\n                    setattr(self, attribute_name, result)\n\n                # If there's registration info, register the output file\n                if len(side_process) &gt; 2 and isinstance(side_process[2], dict):\n                    reg_info = side_process[2]\n\n                    # If an expected_path was provided, use it\n                    if 'expected_path' in reg_info:\n                        file_path = Path(reg_info['expected_path'])\n                    # Otherwise try to get a path from the result\n                    elif isinstance(result, (str, Path)):\n                        file_path = Path(result)\n                    else:\n                        file_path = None\n\n                    if file_path:\n                        # Register the component folder first\n                        self.register_component_folder(\n                            reg_info['component_name'],\n                            reg_info['component_id'],\n                            file_path.parent\n                        )\n                        # Then register the file\n                        self.register_output_file(\n                            reg_info['component_name'],\n                            reg_info['component_id'],\n                            reg_info['file_type'],\n                            file_path\n                        )\n\n        # Clear processed side processes\n        self.side_processes = []\n\n        return self\n</code></pre>"},{"location":"api/data-state/#canopyrs.engine.data_state.DataState.clean_side_processes","title":"<code>clean_side_processes(key=None)</code>","text":"<p>Wait for and process side processes.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>If provided, only process side processes for this attribute.  If None, process all side processes.</p> <code>None</code> Source code in <code>canopyrs/engine/data_state.py</code> <pre><code>def clean_side_processes(self, key: str = None):\n    \"\"\"\n    Wait for and process side processes.\n\n    Args:\n        key: If provided, only process side processes for this attribute.\n             If None, process all side processes.\n    \"\"\"\n    to_process = []\n    to_keep = []\n\n    for side_process in self.side_processes:\n        if key is None or (isinstance(side_process, tuple) and side_process[0] == key):\n            to_process.append(side_process)\n        else:\n            to_keep.append(side_process)\n\n    self.side_processes = to_keep\n\n    for side_process in to_process:\n        if isinstance(side_process, tuple):\n            attribute_name = side_process[0]\n            future_or_result = side_process[1]\n\n            # Check if this is a Future object with a .result() method\n            if hasattr(future_or_result, 'result'):\n                result = future_or_result.result()\n            else:\n                result = future_or_result  # It's already a result\n\n            # Update the data_state attribute\n            if attribute_name:\n                setattr(self, attribute_name, result)\n\n            # If there's registration info, register the output file\n            if len(side_process) &gt; 2 and isinstance(side_process[2], dict):\n                reg_info = side_process[2]\n\n                # If an expected_path was provided, use it\n                if 'expected_path' in reg_info:\n                    file_path = Path(reg_info['expected_path'])\n                # Otherwise try to get a path from the result\n                elif isinstance(result, (str, Path)):\n                    file_path = Path(result)\n                else:\n                    file_path = None\n\n                if file_path:\n                    # Register the component folder first\n                    self.register_component_folder(\n                        reg_info['component_name'],\n                        reg_info['component_id'],\n                        file_path.parent\n                    )\n                    # Then register the file\n                    self.register_output_file(\n                        reg_info['component_name'],\n                        reg_info['component_id'],\n                        reg_info['file_type'],\n                        file_path\n                    )\n\n    # Clear processed side processes\n    self.side_processes = []\n\n    return self\n</code></pre>"},{"location":"api/data-state/#canopyrs.engine.data_state.DataState.get_all_outputs","title":"<code>get_all_outputs()</code>","text":"<p>Get all registered output files organized by component.</p> Source code in <code>canopyrs/engine/data_state.py</code> <pre><code>def get_all_outputs(self) -&gt; Dict:\n    \"\"\"Get all registered output files organized by component.\"\"\"\n    return self.component_output_files\n</code></pre>"},{"location":"api/data-state/#canopyrs.engine.data_state.DataState.get_component_folder","title":"<code>get_component_folder(component_name, component_id)</code>","text":"<p>Get the output folder for a specific component.</p> Source code in <code>canopyrs/engine/data_state.py</code> <pre><code>def get_component_folder(self, component_name: str, component_id: int) -&gt; Optional[Path]:\n    \"\"\"Get the output folder for a specific component.\"\"\"\n    key = get_component_folder_name(component_id, component_name)\n    return self.component_output_folders.get(key)\n</code></pre>"},{"location":"api/data-state/#canopyrs.engine.data_state.DataState.get_latest_output_by_type","title":"<code>get_latest_output_by_type(file_type)</code>","text":"<p>Get the most recent output file of a specific type from any component.</p> Source code in <code>canopyrs/engine/data_state.py</code> <pre><code>def get_latest_output_by_type(self, file_type: str) -&gt; Optional[Path]:\n    \"\"\"Get the most recent output file of a specific type from any component.\"\"\"\n    latest_id = -1\n    latest_path = None\n\n    for key, files in self.component_output_files.items():\n        if file_type in files:\n            component_id = int(key.split('_')[0])\n            if component_id &gt; latest_id:\n                latest_id = component_id\n                latest_path = files[file_type]\n\n    return latest_path\n</code></pre>"},{"location":"api/data-state/#canopyrs.engine.data_state.DataState.get_output_file","title":"<code>get_output_file(component_name, component_id, file_type)</code>","text":"<p>Get a specific output file from a component.</p> Source code in <code>canopyrs/engine/data_state.py</code> <pre><code>def get_output_file(self, component_name: str, component_id: int, file_type: str) -&gt; Optional[Path]:\n    \"\"\"Get a specific output file from a component.\"\"\"\n    key = get_component_folder_name(component_id, component_name)\n    if key in self.component_output_files:\n        return self.component_output_files[key].get(file_type)\n    return None\n</code></pre>"},{"location":"api/data-state/#canopyrs.engine.data_state.DataState.register_component_folder","title":"<code>register_component_folder(component_name, component_id, folder_path)</code>","text":"<p>Register the output folder for a component.</p> Source code in <code>canopyrs/engine/data_state.py</code> <pre><code>def register_component_folder(self, component_name: str, component_id: int, folder_path: Path) -&gt; None:\n    \"\"\"\n    Register the output folder for a component.\n    \"\"\"\n    key = get_component_folder_name(component_id, component_name)\n    self.component_output_folders[key] = folder_path\n</code></pre>"},{"location":"api/data-state/#canopyrs.engine.data_state.DataState.register_output_file","title":"<code>register_output_file(component_name, component_id, file_type, file_path)</code>","text":"<p>Register an output file created by a component.</p> Source code in <code>canopyrs/engine/data_state.py</code> <pre><code>def register_output_file(self, component_name: str, component_id: int, file_type: str, file_path: Path) -&gt; None:\n    \"\"\"\n    Register an output file created by a component.\n    \"\"\"\n    key = get_component_folder_name(component_id, component_name)\n\n    if key not in self.component_output_files:\n        self.component_output_files[key] = {}\n\n    self.component_output_files[key][file_type] = file_path\n</code></pre>"},{"location":"api/pipeline/","title":"Pipeline","text":"<p>Orchestrates component execution with centralized I/O handling.</p> <p>Responsibilities: - Component instantiation and ordering - Pre-run validation of entire pipeline - State management (updating DataState from ComponentResult) - File I/O (saving gpkg, COCO generation) - Output registration - Background task management</p> Source code in <code>canopyrs/engine/pipeline.py</code> <pre><code>class Pipeline:\n    \"\"\"\n    Orchestrates component execution with centralized I/O handling.\n\n    Responsibilities:\n    - Component instantiation and ordering\n    - Pre-run validation of entire pipeline\n    - State management (updating DataState from ComponentResult)\n    - File I/O (saving gpkg, COCO generation)\n    - Output registration\n    - Background task management\n    \"\"\"\n\n    def __init__(\n        self,\n        components: List[BaseComponent],\n        data_state: DataState,\n        output_path: Path,\n        verbose: bool = True,\n    ):\n        \"\"\"\n        Initialize pipeline.\n\n        Args:\n            components: List of component instances (already configured)\n            data_state: Initial data state\n            output_path: Base output directory\n            verbose: Whether to print the flow chart and status messages\n        \"\"\"\n        self.components = components\n        self.data_state = data_state\n        self.output_path = Path(output_path)\n        self.verbose = verbose\n        self.output_path.mkdir(parents=True, exist_ok=True)\n\n        # Setup background executor for async COCO generation\n        self.background_executor = ProcessPoolExecutor(max_workers=1)\n        self.data_state.background_executor = self.background_executor\n\n        # Assign component IDs and output paths\n        for i, component in enumerate(self.components):\n            component.component_id = i\n            component.output_path = self.output_path / f\"{i}_{component.name}\"\n\n        # Validate pipeline configuration immediately to catch errors early\n        if self.verbose:\n            self._print_flow_chart()\n        self._validate_pipeline(raise_on_error=True)\n\n    @classmethod\n    def from_config(cls, io_config: InferIOConfig, config: PipelineConfig, verbose: bool = True) -&gt; 'Pipeline':\n        \"\"\"\n        Create a Pipeline from configuration objects.\n\n        This matches the interface of the original pipeline.py for backward compatibility.\n\n        Args:\n            io_config: Input/output configuration\n            config: Pipeline configuration with component configs\n            verbose: Whether to print the flow chart and status messages\n\n        Returns:\n            Configured Pipeline instance\n        \"\"\"\n        # Import component classes here to avoid circular imports\n        from canopyrs.engine.components.aggregator import AggregatorComponent\n        from canopyrs.engine.components.detector import DetectorComponent\n        from canopyrs.engine.components.segmenter import SegmenterComponent\n        from canopyrs.engine.components.tilerizer import TilerizerComponent\n        from canopyrs.engine.components.classifier import ClassifierComponent\n\n        output_path = Path(io_config.output_folder)\n\n        # Initialize AOI configuration (Area of Interest, used by the Tilerizer)\n        infer_aois_config = parse_tilerizer_aoi_config(\n            aoi_config=io_config.aoi_config,\n            aoi_type=io_config.aoi_type,\n            aois={INFER_AOI_NAME: io_config.aoi}\n        )\n\n        # Instantiate components from config\n        components = []\n        for component_id, (component_type, component_config) in enumerate(config.components_configs):\n            if component_type == 'tilerizer':\n                component = TilerizerComponent(\n                    component_config, output_path, component_id, infer_aois_config\n                )\n            elif component_type == 'detector':\n                component = DetectorComponent(component_config, output_path, component_id)\n            elif component_type == 'aggregator':\n                component = AggregatorComponent(component_config, output_path, component_id)\n            elif component_type == 'segmenter':\n                component = SegmenterComponent(component_config, output_path, component_id)\n            elif component_type == 'classifier':\n                component = ClassifierComponent(component_config, output_path, component_id)\n            else:\n                raise ValueError(f'Invalid component type: {component_type}')\n            components.append(component)\n\n        # Initialize data state from the io (input/output) config\n        infer_gdf = gpd.read_file(io_config.input_gpkg) if io_config.input_gpkg else None\n        infer_gdf_columns_to_pass = (\n            set(io_config.infer_gdf_columns_to_pass)\n            if io_config.infer_gdf_columns_to_pass else set()\n        )\n\n        # If an infer_gdf from a previous pipeline run is provided,\n        # make sure to pass the special columns if present\n        for special_column_name in [object_id_column_name, tile_path_column_name]:\n            if infer_gdf is not None and special_column_name in infer_gdf.columns:\n                infer_gdf_columns_to_pass.add(special_column_name)\n\n        # Derive product name from imagery path, or use default for tiled input\n        if io_config.input_imagery:\n            product_name = validate_and_convert_product_name(\n                strip_all_extensions_and_path(Path(io_config.input_imagery))\n            )\n        else:\n            product_name = \"tiled_input\"\n\n        data_state = DataState(\n            imagery_path=io_config.input_imagery,\n            parent_output_path=io_config.output_folder,\n            product_name=product_name,\n            tiles_path=io_config.tiles_path,\n            infer_coco_path=io_config.input_coco,\n            infer_gdf=infer_gdf,\n            infer_gdf_columns_to_pass=infer_gdf_columns_to_pass,\n        )\n\n        green_print(\"Pipeline initialized\")\n\n        return cls(\n            components=components,\n            data_state=data_state,\n            output_path=output_path,\n            verbose=verbose,\n        )\n\n    # -------------------------------------------------------------------------\n    # Execution\n    # -------------------------------------------------------------------------\n\n    def run(self, strict_rgb_validation: bool = True) -&gt; DataState:\n        \"\"\"\n        Run the pipeline.\n\n        Args:\n            strict_rgb_validation: If True, enforce strict RGB band validation\n\n        Returns:\n            Final DataState with all outputs\n        \"\"\"\n\n        # Validate input raster/tiles bands\n        self._validate_input_data(strict_rgb_validation)\n\n        try:\n            for component in self.components:\n                green_print(f\"Running {component.name}...\")\n\n                self._wait_for_required_state(component)\n\n                # Run component - returns ComponentResult\n                component.output_path.mkdir(parents=True, exist_ok=True)\n                result = component(self.data_state)\n\n                # Pipeline handles all I/O and state updates\n                self._process_result(component, result)\n\n            # Final cleanup of async tasks\n            self.data_state.clean_side_processes()\n\n            # Save final GDF to root output folder if one was produced\n            if self.data_state.infer_gdf is not None:\n                green_print(\"Saving final GeoDataFrame...\")\n                final_gpkg_path = self._save_final_gpkg()\n                num_polygons = len(self.data_state.infer_gdf)\n                print(f\"Final GDF containing {num_polygons} polygons saved to: {final_gpkg_path}\")\n\n            green_print(\"Pipeline finished\")\n\n        finally:\n            self.background_executor.shutdown(wait=True)\n\n        return self.data_state\n\n    def __call__(self) -&gt; DataState:\n        \"\"\"Alias for run().\"\"\"\n        return self.run()\n\n    def _wait_for_required_state(self, component: BaseComponent) -&gt; None:\n        \"\"\"Wait for any required state still being produced by background processes.\"\"\"\n        if self.data_state.side_processes is None or len(self.data_state.side_processes) == 0:\n            return\n        for key in component.requires_state:\n            if getattr(self.data_state, key, None) is None:\n                self.data_state.clean_side_processes(key)\n\n    def _process_result(self, component: BaseComponent, result: ComponentResult):\n        \"\"\"\n        Process ComponentResult: update state and handle I/O.\n\n        This is where all I/O and merging is centralized.\n        \"\"\"\n        # Register component folder\n        self.data_state.register_component_folder(\n            component.name, component.component_id, component.output_path\n        )\n\n        # Apply state updates (e.g., tiles_path, infer_coco_path)\n        for key, value in result.state_updates.items():\n            setattr(self.data_state, key, value)\n\n        # Merge GDF if provided (this replaces the old update_infer_gdf call)\n        if result.gdf is not None:\n            if len(result.gdf) &gt; 0:\n                if result.objects_are_new:\n                    # New objects: replace existing GDF entirely\n                    self.data_state.infer_gdf = self._set_as_new_gdf(result.gdf)\n                else:\n                    # Existing objects refined: merge into existing GDF\n                    merged_gdf = self._merge_result_gdf(result.gdf)\n                    self.data_state.infer_gdf = merged_gdf\n            else:\n                warnings.warn(f\"{component.name} returned an empty GeoDataFrame (0 results).\")\n                self.data_state.infer_gdf = result.gdf\n\n        # Update columns to pass based on what's actually in the merged GDF\n        # (not just what the component claims to produce, since merge may add columns)\n        if self.data_state.infer_gdf is not None:\n            # Use all non-geometry columns from the merged GDF\n            actual_columns = set(self.data_state.infer_gdf.columns) - {Col.GEOMETRY}\n            self.data_state.infer_gdf_columns_to_pass = actual_columns\n\n        # Register any files the component already wrote itself (e.g. geodataset internals)\n        for file_type, file_path in result.output_files.items():\n            if file_path is not None:\n                self.data_state.register_output_file(\n                    component.name, component.component_id, file_type, Path(file_path)\n                )\n\n        # Save GeoPackage if requested (use merged GDF from data_state)\n        if result.save_gpkg and self.data_state.infer_gdf is not None and len(self.data_state.infer_gdf) &gt; 0:\n            gpkg_path = self._save_gpkg(component, self.data_state.infer_gdf, result.gpkg_name_suffix)\n            file_type = 'gpkg' if result.gpkg_name_suffix == 'aggregated' else 'pre_aggregated_gpkg'\n            self.data_state.register_output_file(\n                component.name, component.component_id, file_type, gpkg_path\n            )\n\n        # Queue COCO generation if requested (use merged GDF from data_state)\n        if result.save_coco and self.data_state.infer_gdf is not None and len(self.data_state.infer_gdf) &gt; 0:\n            future_coco = self._queue_coco_generation(component, result)\n            if future_coco:\n                self.data_state.side_processes.append(future_coco)\n\n    def _merge_result_gdf(self, result_gdf: Union[gpd.GeoDataFrame, pd.DataFrame]) -&gt; gpd.GeoDataFrame:\n        \"\"\"\n        Merge component output with existing infer_gdf (objects_are_new=False).\n\n        Only called when a component refines existing objects (e.g., prompted\n        segmenter replacing detector boxes with masks, or aggregator filtering).\n\n        Rules:\n        1. No existing GDF \u2192 set directly, assign object_ids if missing\n        2. Result has geometry + valid merge key \u2192 becomes base, merge in other columns\n        3. Result has geometry + no merge key \u2192 full replacement\n        4. Result has no geometry \u2192 merge attributes into existing\n        5. Merge key priority: object_id &gt; tile_path (if unique)\n\n        Args:\n            result_gdf: Output from component (GeoDataFrame or DataFrame)\n\n        Returns:\n            Merged GeoDataFrame\n        \"\"\"\n        existing_gdf = self.data_state.infer_gdf\n\n        # Case 1: No existing GDF - set directly\n        if existing_gdf is None:\n            return self._set_as_new_gdf(result_gdf)\n\n        # Check if result has geometry\n        has_geometry = Col.GEOMETRY in result_gdf.columns and result_gdf[Col.GEOMETRY].notna().any()\n\n        # Try to determine merge key\n        merge_key = self._determine_merge_key(result_gdf, existing_gdf, raise_on_error=False)\n\n        # Case 2: Result has geometry\n        if has_geometry:\n            if merge_key:\n                # Merge with existing to get other columns\n                return self._merge_with_new_geometry(result_gdf, existing_gdf, merge_key)\n            else:\n                # No merge key - full replacement\n                return self._set_as_new_gdf(result_gdf)\n\n        # Case 3: Result has no geometry - must merge into existing\n        if not merge_key:\n            raise ValueError(\n                \"Cannot merge DataFrame into existing GDF: no valid merge key found. \"\n                f\"Result columns: {list(result_gdf.columns)}\"\n            )\n\n        # no new geometry, merge attributes into existing\n        return self._merge_into_existing(result_gdf, existing_gdf, merge_key)\n\n    def _set_as_new_gdf(self, result_gdf: Union[gpd.GeoDataFrame, pd.DataFrame]) -&gt; gpd.GeoDataFrame:\n        \"\"\"Set result as new infer_gdf, assigning object_ids if missing.\"\"\"\n        # Assign object_ids if not present\n        if Col.OBJECT_ID not in result_gdf.columns:\n            result_gdf = result_gdf.copy()\n            result_gdf[Col.OBJECT_ID] = range(len(result_gdf))\n\n        # Ensure it's a GeoDataFrame\n        if isinstance(result_gdf, gpd.GeoDataFrame):\n            return result_gdf\n\n        # Convert DataFrame to GeoDataFrame (no geometry)\n        if Col.GEOMETRY in result_gdf.columns:\n            return gpd.GeoDataFrame(result_gdf, geometry=Col.GEOMETRY)\n        return gpd.GeoDataFrame(result_gdf)\n\n    def _determine_merge_key(\n        self,\n        result_gdf: Union[gpd.GeoDataFrame, pd.DataFrame],\n        existing_gdf: gpd.GeoDataFrame,\n        raise_on_error: bool = True\n    ) -&gt; Optional[str]:\n        \"\"\"Determine which column to use for merging.\"\"\"\n        # Try object_id first\n        if (Col.OBJECT_ID in result_gdf.columns and\n            Col.OBJECT_ID in existing_gdf.columns and\n            result_gdf[Col.OBJECT_ID].notna().all()):\n            return Col.OBJECT_ID\n\n        # Fall back to tile_path if unique in result\n        if (Col.TILE_PATH in result_gdf.columns and\n            Col.TILE_PATH in existing_gdf.columns and\n            result_gdf[Col.TILE_PATH].is_unique):\n            return Col.TILE_PATH\n\n        if raise_on_error:\n            raise ValueError(\n                \"Cannot merge GDFs: need valid object_id or unique tile_path. \"\n                f\"Result columns: {list(result_gdf.columns)}, \"\n                f\"Existing columns: {list(existing_gdf.columns)}\"\n            )\n        return None\n\n    def _merge_with_new_geometry(\n        self,\n        result_gdf: gpd.GeoDataFrame,\n        existing_gdf: gpd.GeoDataFrame,\n        merge_key: str\n    ) -&gt; gpd.GeoDataFrame:\n        \"\"\"Merge when result has new geometry (e.g., segmenter masks replace detector boxes).\"\"\"\n        # Get columns from existing that aren't in result (except geometry)\n        existing_cols_to_keep = [merge_key]\n        for col in existing_gdf.columns:\n            if col not in result_gdf.columns and col != Col.GEOMETRY:\n                existing_cols_to_keep.append(col)\n\n        # Merge: result is base, pull in other columns from existing\n        merged = result_gdf.merge(\n            existing_gdf[existing_cols_to_keep],\n            on=merge_key,\n            how='left'\n        )\n\n        # Warn about unmatched rows\n        unmatched = merged[merge_key].isna().sum() if merge_key in merged.columns else 0\n        if unmatched &gt; 0:\n            warnings.warn(f\"{unmatched} rows in result had no match in existing GDF\")\n\n        # Check if merge_key is unique in merged. If merge key is OBJECT_ID, warn and assign new unique object_ids to duplicates. If merge key is TILE_PATH, raise error.\n        if merged[merge_key].duplicated().any():\n            if merge_key == Col.OBJECT_ID:\n                warnings.warn(f\"Duplicate OBJECT_IDs found in merged GDF. Assigning new unique OBJECT_IDs. This can happen if a component duplicated objects, like a labeled tilerizer with overlap &gt; 0.\")\n                merged = merged.copy()\n                # Identify duplicates (keep='first' ensures the first occurrence retains its ID)\n                duplicates_mask = merged.duplicated(subset=[Col.OBJECT_ID], keep='first')\n                # Find the current highest ID to start incrementing from\n                current_max_id = merged[Col.OBJECT_ID].max()\n                num_duplicates = duplicates_mask.sum()\n                # Generate a range of new IDs\n                new_ids = range(current_max_id + 1, current_max_id + 1 + num_duplicates)\n                # Assign new IDs only to the rows identified as duplicates\n                merged.loc[duplicates_mask, Col.OBJECT_ID] = new_ids\n            else: \n                raise ValueError(f\"Duplicate TILE_PATHs found in merged GDF after merging. This should not happen as TILE_PATH is the merge_key and is expected to be unique.\")\n\n        return gpd.GeoDataFrame(merged, geometry=Col.GEOMETRY, crs=result_gdf.crs)\n\n    def _merge_into_existing(\n        self,\n        result_gdf: Union[gpd.GeoDataFrame, pd.DataFrame],\n        existing_gdf: gpd.GeoDataFrame,\n        merge_key: str\n    ) -&gt; gpd.GeoDataFrame:\n        \"\"\"Merge attributes into existing GDF (e.g., classifier adds scores).\"\"\"\n        # Drop geometry from result if present (we keep existing geometry)\n        result_cols = [col for col in result_gdf.columns if col != Col.GEOMETRY]\n        result_data = result_gdf[result_cols]\n\n        # Merge: existing is base, add new columns from result\n        merged = existing_gdf.merge(\n            result_data,\n            on=merge_key,\n            how='left'\n        )\n\n        # Warn about unmatched rows\n        new_cols = [col for col in result_cols if col != merge_key]\n        if new_cols:\n            unmatched = merged[new_cols[0]].isna().sum()\n            if unmatched &gt; 0:\n                warnings.warn(f\"{unmatched} rows in existing GDF had no match in result\")\n\n        return gpd.GeoDataFrame(merged, geometry=Col.GEOMETRY, crs=existing_gdf.crs)\n\n    # -------------------------------------------------------------------------\n    # File I/O\n    # -------------------------------------------------------------------------\n\n    def _save_gpkg(self, component: BaseComponent, gdf: gpd.GeoDataFrame, suffix: str) -&gt; Path:\n        \"\"\"Save GeoDataFrame to GeoPackage.\"\"\"\n        gpkg_name = self._generate_gpkg_name(suffix)\n        gpkg_path = component.output_path / gpkg_name\n        gdf.to_file(gpkg_path, driver='GPKG')\n        return gpkg_path\n\n    def _save_final_gpkg(self) -&gt; Path:\n        \"\"\"Save final GeoDataFrame to root output folder.\"\"\"\n        gpkg_name = self._generate_gpkg_name(\"final\")\n        gpkg_path = self.output_path / gpkg_name\n        self.data_state.infer_gdf.to_file(gpkg_path, driver='GPKG')\n        return gpkg_path\n\n    def _generate_gpkg_name(self, suffix: str) -&gt; str:\n        \"\"\"Generate GeoPackage filename using product name from data state.\"\"\"\n        product_name = self.data_state.product_name or \"output\"\n        fold = f\"{INFER_AOI_NAME}{suffix}\" if suffix else INFER_AOI_NAME\n        return GeoPackageNameConvention.create_name(\n            product_name=product_name,\n            fold=fold,\n            scale_factor=1.0,\n            ground_resolution=None\n        )\n\n    def _queue_coco_generation(self, component: BaseComponent, result: ComponentResult):\n        \"\"\"Queue async COCO generation.\"\"\"\n        return generate_future_coco(\n            future_key=StateKey.INFER_COCO_PATH,\n            executor=self.background_executor,\n            component_name=component.name,\n            component_id=component.component_id,\n            description=f\"{component.name} inference\",\n            gdf=self.data_state.infer_gdf,  # Use merged GDF from data_state\n            tiles_paths_column=Col.TILE_PATH,\n            polygons_column=Col.GEOMETRY,\n            scores_column=result.coco_scores_column,\n            categories_column=result.coco_categories_column,\n            other_attributes_columns=result.produced_columns - {Col.GEOMETRY, Col.TILE_PATH},\n            output_path=component.output_path,\n            use_rle_for_labels=False,\n            n_workers=4,\n            coco_categories_list=None\n        )\n\n    # -------------------------------------------------------------------------\n    # Validation\n    # -------------------------------------------------------------------------\n\n    def _validate_input_data(self, strict_rgb_validation: bool = True) -&gt; None:\n        \"\"\"\n        Validate input raster/tiles at pipeline start.\n\n        Uses utility functions from raster_validation module to check RGB band properties.\n\n        Args:\n            strict_rgb_validation: If True, raise error for missing color interpretation.\n                                    If False, only warn if color interpretation is not R,G,B.\n\n        Raises:\n            PipelineValidationError: If validation fails\n        \"\"\"\n        try:\n            if self.data_state.imagery_path:\n                print(\"Validating input raster bands...\")\n                validate_input_raster_or_tiles(\n                    imagery_path=self.data_state.imagery_path,\n                    strict_color_interp=strict_rgb_validation\n                )\n                print(\"Input raster validation passed\")\n            elif self.data_state.tiles_path:\n                print(\"Validating input tiles...\")\n                validate_input_raster_or_tiles(\n                    tiles_path=self.data_state.tiles_path,\n                    strict_color_interp=strict_rgb_validation\n                )\n                print(\"Tile validation passed\")\n        except RasterValidationError as e:\n            # Convert to PipelineValidationError for consistency\n            raise PipelineValidationError(str(e))\n\n    def _validate_pipeline(self, raise_on_error: bool = True) -&gt; List[str]:\n        \"\"\"\n        Validate entire pipeline before running.\n\n        Simulates state flow through components to catch errors early.\n\n        Args:\n            raise_on_error: If True, raise PipelineValidationError on first error\n\n        Returns:\n            List of all validation errors (empty if valid)\n        \"\"\"\n        all_errors = []\n\n        # Start with initial state\n        available_state = self._get_initial_state_keys()\n        available_columns = self._get_initial_columns()\n\n        # Simulate running through each component\n        for i, component in enumerate(self.components):\n            # Validate component\n            errors = component.validate(\n                available_state=available_state,\n                available_columns=available_columns,\n                raise_on_error=False,\n            )\n\n            if errors:\n                all_errors.append(f\"Component {i} ({component.name}):\")\n                all_errors.extend(f\"  {e}\" for e in errors)\n\n            # Update available state/columns with what this component produces\n            available_state = available_state | component.produces_state\n            available_columns = available_columns | component.produces_columns\n\n        if all_errors and raise_on_error:\n            error_msg = \"Pipeline validation failed:\\n\" + \"\\n\".join(all_errors)\n            raise PipelineValidationError(error_msg)\n\n        return all_errors\n\n    def _print_flow_chart(self) -&gt; None:\n        \"\"\"\n        Print a flow chart showing state/column availability through the pipeline.\n\n        This visualizes the data flow through all components, showing what each\n        component requires, produces, and what passes through.\n        \"\"\"\n        visualizer = PipelineFlowVisualizer(\n            components=self.components,\n            initial_state_keys=self._get_initial_state_keys(),\n            initial_columns=self._get_initial_columns(),\n        )\n        visualizer.print()\n\n    def _get_initial_state_keys(self) -&gt; Set[str]:\n        \"\"\"Get state keys available at pipeline start.\"\"\"\n        available = set()\n        if self.data_state.imagery_path:\n            available.add(StateKey.IMAGERY_PATH)\n        if self.data_state.tiles_path:\n            available.add(StateKey.TILES_PATH)\n        if self.data_state.infer_gdf is not None:\n            available.add(StateKey.INFER_GDF)\n        if self.data_state.infer_coco_path:\n            available.add(StateKey.INFER_COCO_PATH)\n        if self.data_state.product_name:\n            available.add(StateKey.PRODUCT_NAME)\n        return available\n\n    def _get_initial_columns(self) -&gt; Set[str]:\n        \"\"\"Get GDF columns available at pipeline start.\"\"\"\n        if self.data_state.infer_gdf is not None:\n            return set(self.data_state.infer_gdf.columns)\n        return set()\n</code></pre> <p>Run a single component standalone (wraps it in a Pipeline).</p> <p>This is a convenience function for users who want to run a single component without manually creating a Pipeline. For a more discoverable API with explicit signatures, use each component's <code>run_standalone()</code> classmethod instead.</p> <p>Parameters:</p> Name Type Description Default <code>component</code> <code>BaseComponent</code> <p>The component to run</p> required <code>output_path</code> <code>str</code> <p>Where to save outputs (required)</p> <code>None</code> <code>imagery_path</code> <code>str</code> <p>Path to imagery (for tilerizer)</p> <code>None</code> <code>tiles_path</code> <code>str</code> <p>Path to tiles (for detector, segmenter)</p> <code>None</code> <code>infer_gdf</code> <code>GeoDataFrame</code> <p>Input GeoDataFrame (for aggregator, classifier)</p> <code>None</code> <code>infer_coco_path</code> <code>str</code> <p>Path to COCO file (for segmenter, classifier)</p> <code>None</code> <code>product_name</code> <code>str</code> <p>Name for output files (derived from imagery if not provided)</p> <code>None</code> <code>**kwargs</code> <p>Additional DataState attributes</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataState</code> <p>DataState with component outputs</p> Example <p>from canopyrs.engine.components.detector import DetectorComponent from canopyrs.engine.config_parsers import DetectorConfig</p> <p>config = DetectorConfig(model='faster_rcnn_detectron2', ...) detector = DetectorComponent(config)</p> <p>result = run_component(     detector,     output_path='./output',     tiles_path='./tiles' ) print(result.infer_gdf)</p> Source code in <code>canopyrs/engine/pipeline.py</code> <pre><code>def run_component(\n    component: BaseComponent,\n    output_path: str = None,\n    imagery_path: str = None,\n    tiles_path: str = None,\n    infer_gdf: gpd.GeoDataFrame = None,\n    infer_coco_path: str = None,\n    product_name: str = None,\n    **kwargs\n) -&gt; DataState:\n    \"\"\"\n    Run a single component standalone (wraps it in a Pipeline).\n\n    This is a convenience function for users who want to run a single\n    component without manually creating a Pipeline. For a more discoverable\n    API with explicit signatures, use each component's ``run_standalone()``\n    classmethod instead.\n\n    Args:\n        component: The component to run\n        output_path: Where to save outputs (required)\n        imagery_path: Path to imagery (for tilerizer)\n        tiles_path: Path to tiles (for detector, segmenter)\n        infer_gdf: Input GeoDataFrame (for aggregator, classifier)\n        infer_coco_path: Path to COCO file (for segmenter, classifier)\n        product_name: Name for output files (derived from imagery if not provided)\n        **kwargs: Additional DataState attributes\n\n    Returns:\n        DataState with component outputs\n\n    Example:\n        from canopyrs.engine.components.detector import DetectorComponent\n        from canopyrs.engine.config_parsers import DetectorConfig\n\n        config = DetectorConfig(model='faster_rcnn_detectron2', ...)\n        detector = DetectorComponent(config)\n\n        result = run_component(\n            detector,\n            output_path='./output',\n            tiles_path='./tiles'\n        )\n        print(result.infer_gdf)\n    \"\"\"\n    if output_path is None:\n        raise ValueError(\"output_path is required for run_component()\")\n\n    # Validate inputs against component requirements before creating Pipeline\n    available_state = {\n        key for key, value in {\n            StateKey.IMAGERY_PATH: imagery_path,\n            StateKey.TILES_PATH: tiles_path,\n            StateKey.INFER_GDF: infer_gdf,\n            StateKey.INFER_COCO_PATH: infer_coco_path,\n            StateKey.PRODUCT_NAME: product_name,\n        }.items() if value is not None\n    }\n    available_columns = set(infer_gdf.columns) if infer_gdf is not None else set()\n\n    errors = component.validate(\n        available_state=available_state,\n        available_columns=available_columns,\n        raise_on_error=False,\n    )\n    if errors:\n        error_msg = (\n            f\"Cannot run '{component.name}' standalone - missing inputs:\\n\"\n            + \"\\n\".join(f\"  * {e}\" for e in errors)\n            + f\"\\n\\n{component.describe()}\"\n        )\n        raise ComponentValidationError(error_msg)\n\n    # Derive product name if not provided\n    if product_name is None:\n        if imagery_path:\n            product_name = validate_and_convert_product_name(\n                strip_all_extensions_and_path(Path(imagery_path))\n            )\n        else:\n            product_name = \"tiled_input\"\n\n    # Create DataState from provided inputs\n    data_state = DataState(\n        imagery_path=imagery_path,\n        tiles_path=tiles_path,\n        product_name=product_name,\n        infer_gdf=infer_gdf,\n        infer_coco_path=infer_coco_path,\n        parent_output_path=output_path,\n        **kwargs\n    )\n\n    # Create and run pipeline with single component\n    pipeline = Pipeline(\n        components=[component],\n        data_state=data_state,\n        output_path=output_path,\n    )\n\n    return pipeline.run()\n</code></pre>"},{"location":"api/pipeline/#canopyrs.engine.pipeline.Pipeline.__call__","title":"<code>__call__()</code>","text":"<p>Alias for run().</p> Source code in <code>canopyrs/engine/pipeline.py</code> <pre><code>def __call__(self) -&gt; DataState:\n    \"\"\"Alias for run().\"\"\"\n    return self.run()\n</code></pre>"},{"location":"api/pipeline/#canopyrs.engine.pipeline.Pipeline.__init__","title":"<code>__init__(components, data_state, output_path, verbose=True)</code>","text":"<p>Initialize pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>components</code> <code>List[BaseComponent]</code> <p>List of component instances (already configured)</p> required <code>data_state</code> <code>DataState</code> <p>Initial data state</p> required <code>output_path</code> <code>Path</code> <p>Base output directory</p> required <code>verbose</code> <code>bool</code> <p>Whether to print the flow chart and status messages</p> <code>True</code> Source code in <code>canopyrs/engine/pipeline.py</code> <pre><code>def __init__(\n    self,\n    components: List[BaseComponent],\n    data_state: DataState,\n    output_path: Path,\n    verbose: bool = True,\n):\n    \"\"\"\n    Initialize pipeline.\n\n    Args:\n        components: List of component instances (already configured)\n        data_state: Initial data state\n        output_path: Base output directory\n        verbose: Whether to print the flow chart and status messages\n    \"\"\"\n    self.components = components\n    self.data_state = data_state\n    self.output_path = Path(output_path)\n    self.verbose = verbose\n    self.output_path.mkdir(parents=True, exist_ok=True)\n\n    # Setup background executor for async COCO generation\n    self.background_executor = ProcessPoolExecutor(max_workers=1)\n    self.data_state.background_executor = self.background_executor\n\n    # Assign component IDs and output paths\n    for i, component in enumerate(self.components):\n        component.component_id = i\n        component.output_path = self.output_path / f\"{i}_{component.name}\"\n\n    # Validate pipeline configuration immediately to catch errors early\n    if self.verbose:\n        self._print_flow_chart()\n    self._validate_pipeline(raise_on_error=True)\n</code></pre>"},{"location":"api/pipeline/#canopyrs.engine.pipeline.Pipeline.from_config","title":"<code>from_config(io_config, config, verbose=True)</code>  <code>classmethod</code>","text":"<p>Create a Pipeline from configuration objects.</p> <p>This matches the interface of the original pipeline.py for backward compatibility.</p> <p>Parameters:</p> Name Type Description Default <code>io_config</code> <code>InferIOConfig</code> <p>Input/output configuration</p> required <code>config</code> <code>PipelineConfig</code> <p>Pipeline configuration with component configs</p> required <code>verbose</code> <code>bool</code> <p>Whether to print the flow chart and status messages</p> <code>True</code> <p>Returns:</p> Type Description <code>Pipeline</code> <p>Configured Pipeline instance</p> Source code in <code>canopyrs/engine/pipeline.py</code> <pre><code>@classmethod\ndef from_config(cls, io_config: InferIOConfig, config: PipelineConfig, verbose: bool = True) -&gt; 'Pipeline':\n    \"\"\"\n    Create a Pipeline from configuration objects.\n\n    This matches the interface of the original pipeline.py for backward compatibility.\n\n    Args:\n        io_config: Input/output configuration\n        config: Pipeline configuration with component configs\n        verbose: Whether to print the flow chart and status messages\n\n    Returns:\n        Configured Pipeline instance\n    \"\"\"\n    # Import component classes here to avoid circular imports\n    from canopyrs.engine.components.aggregator import AggregatorComponent\n    from canopyrs.engine.components.detector import DetectorComponent\n    from canopyrs.engine.components.segmenter import SegmenterComponent\n    from canopyrs.engine.components.tilerizer import TilerizerComponent\n    from canopyrs.engine.components.classifier import ClassifierComponent\n\n    output_path = Path(io_config.output_folder)\n\n    # Initialize AOI configuration (Area of Interest, used by the Tilerizer)\n    infer_aois_config = parse_tilerizer_aoi_config(\n        aoi_config=io_config.aoi_config,\n        aoi_type=io_config.aoi_type,\n        aois={INFER_AOI_NAME: io_config.aoi}\n    )\n\n    # Instantiate components from config\n    components = []\n    for component_id, (component_type, component_config) in enumerate(config.components_configs):\n        if component_type == 'tilerizer':\n            component = TilerizerComponent(\n                component_config, output_path, component_id, infer_aois_config\n            )\n        elif component_type == 'detector':\n            component = DetectorComponent(component_config, output_path, component_id)\n        elif component_type == 'aggregator':\n            component = AggregatorComponent(component_config, output_path, component_id)\n        elif component_type == 'segmenter':\n            component = SegmenterComponent(component_config, output_path, component_id)\n        elif component_type == 'classifier':\n            component = ClassifierComponent(component_config, output_path, component_id)\n        else:\n            raise ValueError(f'Invalid component type: {component_type}')\n        components.append(component)\n\n    # Initialize data state from the io (input/output) config\n    infer_gdf = gpd.read_file(io_config.input_gpkg) if io_config.input_gpkg else None\n    infer_gdf_columns_to_pass = (\n        set(io_config.infer_gdf_columns_to_pass)\n        if io_config.infer_gdf_columns_to_pass else set()\n    )\n\n    # If an infer_gdf from a previous pipeline run is provided,\n    # make sure to pass the special columns if present\n    for special_column_name in [object_id_column_name, tile_path_column_name]:\n        if infer_gdf is not None and special_column_name in infer_gdf.columns:\n            infer_gdf_columns_to_pass.add(special_column_name)\n\n    # Derive product name from imagery path, or use default for tiled input\n    if io_config.input_imagery:\n        product_name = validate_and_convert_product_name(\n            strip_all_extensions_and_path(Path(io_config.input_imagery))\n        )\n    else:\n        product_name = \"tiled_input\"\n\n    data_state = DataState(\n        imagery_path=io_config.input_imagery,\n        parent_output_path=io_config.output_folder,\n        product_name=product_name,\n        tiles_path=io_config.tiles_path,\n        infer_coco_path=io_config.input_coco,\n        infer_gdf=infer_gdf,\n        infer_gdf_columns_to_pass=infer_gdf_columns_to_pass,\n    )\n\n    green_print(\"Pipeline initialized\")\n\n    return cls(\n        components=components,\n        data_state=data_state,\n        output_path=output_path,\n        verbose=verbose,\n    )\n</code></pre>"},{"location":"api/pipeline/#canopyrs.engine.pipeline.Pipeline.run","title":"<code>run(strict_rgb_validation=True)</code>","text":"<p>Run the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>strict_rgb_validation</code> <code>bool</code> <p>If True, enforce strict RGB band validation</p> <code>True</code> <p>Returns:</p> Type Description <code>DataState</code> <p>Final DataState with all outputs</p> Source code in <code>canopyrs/engine/pipeline.py</code> <pre><code>def run(self, strict_rgb_validation: bool = True) -&gt; DataState:\n    \"\"\"\n    Run the pipeline.\n\n    Args:\n        strict_rgb_validation: If True, enforce strict RGB band validation\n\n    Returns:\n        Final DataState with all outputs\n    \"\"\"\n\n    # Validate input raster/tiles bands\n    self._validate_input_data(strict_rgb_validation)\n\n    try:\n        for component in self.components:\n            green_print(f\"Running {component.name}...\")\n\n            self._wait_for_required_state(component)\n\n            # Run component - returns ComponentResult\n            component.output_path.mkdir(parents=True, exist_ok=True)\n            result = component(self.data_state)\n\n            # Pipeline handles all I/O and state updates\n            self._process_result(component, result)\n\n        # Final cleanup of async tasks\n        self.data_state.clean_side_processes()\n\n        # Save final GDF to root output folder if one was produced\n        if self.data_state.infer_gdf is not None:\n            green_print(\"Saving final GeoDataFrame...\")\n            final_gpkg_path = self._save_final_gpkg()\n            num_polygons = len(self.data_state.infer_gdf)\n            print(f\"Final GDF containing {num_polygons} polygons saved to: {final_gpkg_path}\")\n\n        green_print(\"Pipeline finished\")\n\n    finally:\n        self.background_executor.shutdown(wait=True)\n\n    return self.data_state\n</code></pre>"},{"location":"api/components/aggregator/","title":"Aggregator","text":"<p>               Bases: <code>BaseComponent</code></p> <p>Aggregates overlapping detections/segmentations from tiled inference.</p> Requirements <ul> <li>infer_gdf with geometry, object_id, tile_path columns</li> <li>Score columns based on config weights (detector_score/segmenter_score)</li> </ul> Produces <ul> <li>Merged GeoDataFrame with aggregator_score</li> <li>GeoPackage files (aggregated + pre-aggregated)</li> <li>COCO file</li> </ul> Source code in <code>canopyrs/engine/components/aggregator.py</code> <pre><code>class AggregatorComponent(BaseComponent):\n    \"\"\"\n    Aggregates overlapping detections/segmentations from tiled inference.\n\n    Requirements:\n        - infer_gdf with geometry, object_id, tile_path columns\n        - Score columns based on config weights (detector_score/segmenter_score)\n\n    Produces:\n        - Merged GeoDataFrame with aggregator_score\n        - GeoPackage files (aggregated + pre-aggregated)\n        - COCO file\n    \"\"\"\n\n    name = 'aggregator'\n\n    BASE_REQUIRES_STATE = {StateKey.INFER_GDF, StateKey.PRODUCT_NAME}\n    BASE_REQUIRES_COLUMNS = {Col.GEOMETRY, Col.OBJECT_ID, Col.TILE_PATH}\n\n    BASE_PRODUCES_STATE = {StateKey.INFER_GDF, StateKey.INFER_COCO_PATH}\n    BASE_PRODUCES_COLUMNS = {Col.AGGREGATOR_SCORE}\n\n    BASE_STATE_HINTS = {\n        StateKey.INFER_GDF: (\n            \"Aggregator needs a GeoDataFrame with detections/segmentations. \"\n            \"Add a detector or segmenter before aggregator in the pipeline.\"\n        ),\n    }\n\n    BASE_COLUMN_HINTS = {\n        Col.GEOMETRY: \"GeoDataFrame must have a 'geometry' column with polygon geometries.\",\n        Col.OBJECT_ID: \"Each detection needs a unique 'canopyrs_object_id'. Created by detector/segmenter.\",\n        Col.TILE_PATH: \"Each detection needs a 'tile_path' column indicating source tile.\",\n    }\n\n    def __init__(\n        self,\n        config: AggregatorConfig,\n        parent_output_path: str = None,\n        component_id: int = None\n    ):\n        super().__init__(config, parent_output_path, component_id)\n\n        # Set base requirements\n        self.requires_state = set(self.BASE_REQUIRES_STATE)\n        self.requires_columns = set(self.BASE_REQUIRES_COLUMNS)\n        self.produces_state = set(self.BASE_PRODUCES_STATE)\n        self.produces_columns = set(self.BASE_PRODUCES_COLUMNS)\n\n        # Set hints\n        self.state_hints = dict(self.BASE_STATE_HINTS)\n        self.column_hints = dict(self.BASE_COLUMN_HINTS)\n\n        # Add config-dependent requirements\n        if config.detector_score_weight &gt; 0:\n            self.requires_columns.add(Col.DETECTOR_SCORE)\n            self.column_hints[Col.DETECTOR_SCORE] = (\n                f\"Config has detector_score_weight={config.detector_score_weight} &gt; 0, \"\n                f\"so '{Col.DETECTOR_SCORE}' column is required. \"\n                f\"Add a detector before aggregator, or set detector_score_weight=0.\"\n            )\n\n        if config.segmenter_score_weight &gt; 0:\n            self.requires_columns.add(Col.SEGMENTER_SCORE)\n            self.column_hints[Col.SEGMENTER_SCORE] = (\n                f\"Config has segmenter_score_weight={config.segmenter_score_weight} &gt; 0, \"\n                f\"so '{Col.SEGMENTER_SCORE}' column is required. \"\n                f\"Add a segmenter before aggregator, or set segmenter_score_weight=0.\"\n            )\n\n    @classmethod\n    def run_standalone(\n        cls,\n        config: AggregatorConfig,\n        infer_gdf: 'gpd.GeoDataFrame',\n        output_path: str,\n        product_name: str = \"standalone\",\n    ) -&gt; 'DataState':\n        \"\"\"\n        Run aggregator standalone on a GeoDataFrame of detections/segmentations.\n\n        Args:\n            config: Aggregator configuration\n            infer_gdf: GeoDataFrame with geometry, object_id, and tile_path columns\n            output_path: Where to save outputs\n            product_name: Name for output files (used in gpkg naming)\n\n        Returns:\n            DataState with aggregated results (access .infer_gdf for the GeoDataFrame)\n\n        Example:\n            result = AggregatorComponent.run_standalone(\n                config=AggregatorConfig(nms_threshold=0.5, ...),\n                infer_gdf=my_detections_gdf,\n                output_path='./output',\n            )\n            print(result.infer_gdf)\n        \"\"\"\n        from canopyrs.engine.pipeline import run_component\n        return run_component(\n            component=cls(config),\n            output_path=output_path,\n            infer_gdf=infer_gdf,\n            product_name=product_name,\n        )\n\n    @validate_requirements\n    def __call__(self, data_state: DataState) -&gt; ComponentResult:\n        \"\"\"\n        Aggregate overlapping detections/segmentations.\n\n        Returns ComponentResult - Pipeline handles I/O and state updates.\n        \"\"\"\n        # Suppress geographic CRS area warnings from geopandas\n        warnings.filterwarnings('ignore', message='.*Geometry is in a geographic CRS.*')\n\n        infer_gdf = data_state.infer_gdf\n        columns_to_pass = data_state.infer_gdf_columns_to_pass\n\n        # Build score columns and weights from config\n        score_cols = []\n        weights = []\n\n        if self.config.detector_score_weight &gt; 0:\n            score_cols.append(Col.DETECTOR_SCORE)\n            weights.append(self.config.detector_score_weight)\n\n        if self.config.segmenter_score_weight &gt; 0:\n            score_cols.append(Col.SEGMENTER_SCORE)\n            weights.append(self.config.segmenter_score_weight)\n\n        # Generate output names\n        gpkg_name, pre_agg_gpkg_name = self._get_gpkg_names(data_state)\n\n        # Drop some previous components columns that can interfere with aggregation\n        for col in [Col.AGGREGATOR_SCORE, 'tile_id']:\n            if col in infer_gdf.columns:\n                infer_gdf = infer_gdf.drop(columns=[col])\n                if col in columns_to_pass:\n                    columns_to_pass.remove(col)\n\n        # Run aggregation (geodataset Aggregator handles its own file saving)\n        aggregator = Aggregator.from_gdf(\n            output_path=self.output_path / gpkg_name if self.output_path else None,\n            gdf=infer_gdf,\n            tiles_paths_column=Col.TILE_PATH,\n            polygons_column=Col.GEOMETRY,\n            scores_column=score_cols if score_cols else None,\n            other_attributes_columns=list(columns_to_pass),\n            scores_weights=weights if weights else None,\n            scores_weighting_method=self.config.scores_weighting_method,\n            min_centroid_distance_weight=self.config.min_centroid_distance_weight,\n            score_threshold=self.config.score_threshold,\n            nms_threshold=self.config.nms_threshold,\n            nms_algorithm=self.config.nms_algorithm,\n            best_geom_keep_area_ratio=self.config.best_geom_keep_area_ratio,\n            edge_band_buffer_percentage=self.config.edge_band_buffer_percentage,\n            pre_aggregated_output_path=self.output_path / pre_agg_gpkg_name if self.output_path else None,\n        )\n\n        result_gdf = aggregator.polygons_gdf\n\n        # Determine category column for COCO\n        coco_categories_col = None\n        if Col.SEGMENTER_CLASS in result_gdf.columns:\n            coco_categories_col = Col.SEGMENTER_CLASS\n        elif Col.DETECTOR_CLASS in result_gdf.columns:\n            coco_categories_col = Col.DETECTOR_CLASS\n\n        # Register the GeoPackages that geodataset already wrote\n        output_files = {}\n        if self.output_path:\n            output_files['gpkg'] = self.output_path / gpkg_name\n            output_files['pre_aggregated_gpkg'] = self.output_path / pre_agg_gpkg_name\n\n        return ComponentResult(\n            gdf=result_gdf,\n            produced_columns=columns_to_pass | {Col.AGGREGATOR_SCORE},\n            objects_are_new=False,\n            save_gpkg=False,  # Aggregator already saves via geodataset\n            save_coco=True,\n            coco_scores_column=Col.AGGREGATOR_SCORE,\n            coco_categories_column=coco_categories_col,\n            output_files=output_files,\n        )\n\n    def _get_gpkg_names(self, data_state: DataState) -&gt; tuple:\n        \"\"\"Generate GeoPackage names using the product name from data state.\"\"\"\n\n        try:\n            _, scale_factor, ground_resolution, _, _, _ = TileNameConvention().parse_name(\n                Path(data_state.infer_gdf[Col.TILE_PATH].iloc[0]).name\n            )\n        except Exception as e:\n            scale_factor = 1.0\n            ground_resolution = None\n\n        gpkg_name = GeoPackageNameConvention.create_name(\n            product_name=data_state.product_name,\n            fold=INFER_AOI_NAME,\n            scale_factor=scale_factor,\n            ground_resolution=ground_resolution\n        )\n\n        pre_agg_gpkg_name = GeoPackageNameConvention.create_name(\n            product_name=data_state.product_name,\n            fold=f'{INFER_AOI_NAME}notaggregated',\n            scale_factor=scale_factor,\n            ground_resolution=ground_resolution\n        )\n\n        return gpkg_name, pre_agg_gpkg_name\n</code></pre>"},{"location":"api/components/aggregator/#canopyrs.engine.components.aggregator.AggregatorComponent.__call__","title":"<code>__call__(data_state)</code>","text":"<p>Aggregate overlapping detections/segmentations.</p> <p>Returns ComponentResult - Pipeline handles I/O and state updates.</p> Source code in <code>canopyrs/engine/components/aggregator.py</code> <pre><code>@validate_requirements\ndef __call__(self, data_state: DataState) -&gt; ComponentResult:\n    \"\"\"\n    Aggregate overlapping detections/segmentations.\n\n    Returns ComponentResult - Pipeline handles I/O and state updates.\n    \"\"\"\n    # Suppress geographic CRS area warnings from geopandas\n    warnings.filterwarnings('ignore', message='.*Geometry is in a geographic CRS.*')\n\n    infer_gdf = data_state.infer_gdf\n    columns_to_pass = data_state.infer_gdf_columns_to_pass\n\n    # Build score columns and weights from config\n    score_cols = []\n    weights = []\n\n    if self.config.detector_score_weight &gt; 0:\n        score_cols.append(Col.DETECTOR_SCORE)\n        weights.append(self.config.detector_score_weight)\n\n    if self.config.segmenter_score_weight &gt; 0:\n        score_cols.append(Col.SEGMENTER_SCORE)\n        weights.append(self.config.segmenter_score_weight)\n\n    # Generate output names\n    gpkg_name, pre_agg_gpkg_name = self._get_gpkg_names(data_state)\n\n    # Drop some previous components columns that can interfere with aggregation\n    for col in [Col.AGGREGATOR_SCORE, 'tile_id']:\n        if col in infer_gdf.columns:\n            infer_gdf = infer_gdf.drop(columns=[col])\n            if col in columns_to_pass:\n                columns_to_pass.remove(col)\n\n    # Run aggregation (geodataset Aggregator handles its own file saving)\n    aggregator = Aggregator.from_gdf(\n        output_path=self.output_path / gpkg_name if self.output_path else None,\n        gdf=infer_gdf,\n        tiles_paths_column=Col.TILE_PATH,\n        polygons_column=Col.GEOMETRY,\n        scores_column=score_cols if score_cols else None,\n        other_attributes_columns=list(columns_to_pass),\n        scores_weights=weights if weights else None,\n        scores_weighting_method=self.config.scores_weighting_method,\n        min_centroid_distance_weight=self.config.min_centroid_distance_weight,\n        score_threshold=self.config.score_threshold,\n        nms_threshold=self.config.nms_threshold,\n        nms_algorithm=self.config.nms_algorithm,\n        best_geom_keep_area_ratio=self.config.best_geom_keep_area_ratio,\n        edge_band_buffer_percentage=self.config.edge_band_buffer_percentage,\n        pre_aggregated_output_path=self.output_path / pre_agg_gpkg_name if self.output_path else None,\n    )\n\n    result_gdf = aggregator.polygons_gdf\n\n    # Determine category column for COCO\n    coco_categories_col = None\n    if Col.SEGMENTER_CLASS in result_gdf.columns:\n        coco_categories_col = Col.SEGMENTER_CLASS\n    elif Col.DETECTOR_CLASS in result_gdf.columns:\n        coco_categories_col = Col.DETECTOR_CLASS\n\n    # Register the GeoPackages that geodataset already wrote\n    output_files = {}\n    if self.output_path:\n        output_files['gpkg'] = self.output_path / gpkg_name\n        output_files['pre_aggregated_gpkg'] = self.output_path / pre_agg_gpkg_name\n\n    return ComponentResult(\n        gdf=result_gdf,\n        produced_columns=columns_to_pass | {Col.AGGREGATOR_SCORE},\n        objects_are_new=False,\n        save_gpkg=False,  # Aggregator already saves via geodataset\n        save_coco=True,\n        coco_scores_column=Col.AGGREGATOR_SCORE,\n        coco_categories_column=coco_categories_col,\n        output_files=output_files,\n    )\n</code></pre>"},{"location":"api/components/aggregator/#canopyrs.engine.components.aggregator.AggregatorComponent.run_standalone","title":"<code>run_standalone(config, infer_gdf, output_path, product_name='standalone')</code>  <code>classmethod</code>","text":"<p>Run aggregator standalone on a GeoDataFrame of detections/segmentations.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>AggregatorConfig</code> <p>Aggregator configuration</p> required <code>infer_gdf</code> <code>GeoDataFrame</code> <p>GeoDataFrame with geometry, object_id, and tile_path columns</p> required <code>output_path</code> <code>str</code> <p>Where to save outputs</p> required <code>product_name</code> <code>str</code> <p>Name for output files (used in gpkg naming)</p> <code>'standalone'</code> <p>Returns:</p> Type Description <code>DataState</code> <p>DataState with aggregated results (access .infer_gdf for the GeoDataFrame)</p> Example <p>result = AggregatorComponent.run_standalone(     config=AggregatorConfig(nms_threshold=0.5, ...),     infer_gdf=my_detections_gdf,     output_path='./output', ) print(result.infer_gdf)</p> Source code in <code>canopyrs/engine/components/aggregator.py</code> <pre><code>@classmethod\ndef run_standalone(\n    cls,\n    config: AggregatorConfig,\n    infer_gdf: 'gpd.GeoDataFrame',\n    output_path: str,\n    product_name: str = \"standalone\",\n) -&gt; 'DataState':\n    \"\"\"\n    Run aggregator standalone on a GeoDataFrame of detections/segmentations.\n\n    Args:\n        config: Aggregator configuration\n        infer_gdf: GeoDataFrame with geometry, object_id, and tile_path columns\n        output_path: Where to save outputs\n        product_name: Name for output files (used in gpkg naming)\n\n    Returns:\n        DataState with aggregated results (access .infer_gdf for the GeoDataFrame)\n\n    Example:\n        result = AggregatorComponent.run_standalone(\n            config=AggregatorConfig(nms_threshold=0.5, ...),\n            infer_gdf=my_detections_gdf,\n            output_path='./output',\n        )\n        print(result.infer_gdf)\n    \"\"\"\n    from canopyrs.engine.pipeline import run_component\n    return run_component(\n        component=cls(config),\n        output_path=output_path,\n        infer_gdf=infer_gdf,\n        product_name=product_name,\n    )\n</code></pre>"},{"location":"api/components/base/","title":"Base","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all pipeline components.</p> <p>Components have a single interface: - call(data_state) -&gt; ComponentResult</p> <p>Pipeline handles: - State updates (based on ComponentResult) - File I/O (saving gpkg, COCO generation) - Output registration</p> <p>Requirements are declared as class/instance attributes: - requires_state: Set of StateKey values needed - requires_columns: Set of Col values needed in GDF - produces_state: Set of StateKey values produced - produces_columns: Set of Col values produced</p> <p>Optional hints for better error messages: - state_hints: Dict mapping state key to helpful message - column_hints: Dict mapping column name to helpful message</p> Source code in <code>canopyrs/engine/components/base.py</code> <pre><code>class BaseComponent(ABC):\n    \"\"\"\n    Base class for all pipeline components.\n\n    Components have a single interface:\n    - __call__(data_state) -&gt; ComponentResult\n\n    Pipeline handles:\n    - State updates (based on ComponentResult)\n    - File I/O (saving gpkg, COCO generation)\n    - Output registration\n\n    Requirements are declared as class/instance attributes:\n    - requires_state: Set of StateKey values needed\n    - requires_columns: Set of Col values needed in GDF\n    - produces_state: Set of StateKey values produced\n    - produces_columns: Set of Col values produced\n\n    Optional hints for better error messages:\n    - state_hints: Dict mapping state key to helpful message\n    - column_hints: Dict mapping column name to helpful message\n    \"\"\"\n\n    name: str\n\n    # Default empty requirements (subclasses override)\n    requires_state: Set[str] = set()\n    requires_columns: Set[str] = set()\n    produces_state: Set[str] = set()\n    produces_columns: Set[str] = set()\n\n    # Optional hints for validation errors (subclasses can override)\n    state_hints: Dict[str, str] = {}\n    column_hints: Dict[str, str] = {}\n\n    def __init__(\n        self,\n        config,\n        parent_output_path: str = None,\n        component_id: int = None\n    ):\n        self.config = config\n        self.parent_output_path = parent_output_path\n        self.component_id = component_id\n        self.output_path = None  # Set by Pipeline before calling\n\n    # -------------------------------------------------------------------------\n    # Validation\n    # -------------------------------------------------------------------------\n\n    def validate(\n        self,\n        available_state: Set[str] = None,\n        available_columns: Set[str] = None,\n        raise_on_error: bool = True,\n    ) -&gt; List[str]:\n        \"\"\"\n        Validate that this component can run with the given inputs.\n\n        Used by Pipeline before running to catch config errors early.\n\n        Args:\n            available_state: Set of available state keys\n            available_columns: Set of available GDF column names\n            raise_on_error: If True, raise ComponentValidationError\n\n        Returns:\n            List of error messages (empty if valid)\n        \"\"\"\n        errors = []\n        available_state = available_state or set()\n        available_columns = available_columns or set()\n\n        # Check state requirements\n        missing_state = self.requires_state - available_state\n        for key in missing_state:\n            hint = self.state_hints.get(key, \"\")\n            msg = f\"Missing required state: '{key}'\"\n            if hint:\n                msg += f\"\\n    -&gt; Hint: {hint}\"\n            errors.append(msg)\n\n        # Check column requirements\n        missing_columns = self.requires_columns - available_columns\n        for col in missing_columns:\n            hint = self.column_hints.get(col, \"\")\n            msg = f\"Missing required column: '{col}'\"\n            if hint:\n                msg += f\"\\n    -&gt; Hint: {hint}\"\n            errors.append(msg)\n\n        if errors and raise_on_error:\n            error_msg = f\"Component '{self.name}' validation failed:\\n\"\n            error_msg += \"\\n\".join(f\"  * {e}\" for e in errors)\n            raise ComponentValidationError(error_msg)\n\n        return errors\n\n    # -------------------------------------------------------------------------\n    # Abstract Method - Single Interface\n    # -------------------------------------------------------------------------\n\n    @abstractmethod\n    def __call__(self, data_state: DataState) -&gt; ComponentResult:\n        \"\"\"\n        Run the component's core logic.\n\n        Args:\n            data_state: Current pipeline state\n\n        Returns:\n            ComponentResult with outputs and I/O instructions\n        \"\"\"\n        pass\n\n    # -------------------------------------------------------------------------\n    # Utilities\n    # -------------------------------------------------------------------------\n\n    def describe(self) -&gt; str:\n        \"\"\"Return a human-readable description of this component's requirements.\"\"\"\n        lines = [\n            f\"Component: {self.name}\",\n            f\"  Requires state: {self.requires_state or 'none'}\",\n            f\"  Requires columns: {self.requires_columns or 'none'}\",\n            f\"  Produces state: {self.produces_state or 'none'}\",\n            f\"  Produces columns: {self.produces_columns or 'none'}\",\n        ]\n        return \"\\n\".join(lines)\n</code></pre> <p>Output from component call(). Pipeline uses this to update DataState and handle I/O.</p> Source code in <code>canopyrs/engine/components/base.py</code> <pre><code>@dataclass\nclass ComponentResult:\n    \"\"\"\n    Output from component __call__().\n    Pipeline uses this to update DataState and handle I/O.\n    \"\"\"\n    # Core output data\n    gdf: Optional[gpd.GeoDataFrame] = None\n    produced_columns: Set[str] = field(default_factory=set)\n    objects_are_new: bool = True  # If True, GDF replaces existing; if False, merge into existing (if able to)\n\n    # State updates (for non-GDF state like tiles_path)\n    state_updates: Dict[str, Any] = field(default_factory=dict)\n\n    # I/O flags - Pipeline handles actual saving\n    save_gpkg: bool = False\n    gpkg_name_suffix: str = \"notaggregated\"  # e.g., \"notaggregated\", \"aggregated\"\n\n    save_coco: bool = False\n    coco_scores_column: Optional[str] = None\n    coco_categories_column: Optional[str] = None\n\n    # Files already written by the component itself (e.g. geodataset internals).\n    # Pipeline will register these in component_output_files without re-saving.\n    # Keys are file_type strings (e.g. 'coco', 'gpkg', 'pre_aggregated_gpkg').\n    output_files: Dict[str, Any] = field(default_factory=dict)\n</code></pre> <p>Decorator that validates data_state has required state keys and GDF columns at runtime.</p> <p>Checks the component's requires_state and requires_columns against the actual data_state before executing the component.</p> Usage <p>@validate_requirements def call(self, data_state: DataState) -&gt; ComponentResult:     ...</p> Source code in <code>canopyrs/engine/components/base.py</code> <pre><code>def validate_requirements(method: Callable[..., T]) -&gt; Callable[..., T]:\n    \"\"\"\n    Decorator that validates data_state has required state keys and GDF columns at runtime.\n\n    Checks the component's requires_state and requires_columns against\n    the actual data_state before executing the component.\n\n    Usage:\n        @validate_requirements\n        def __call__(self, data_state: DataState) -&gt; ComponentResult:\n            ...\n    \"\"\"\n    @functools.wraps(method)\n    def wrapper(self: 'BaseComponent', data_state: DataState) -&gt; 'ComponentResult':\n        # Check state keys (must exist and not be None)\n        missing_state = set()\n        for key in self.requires_state:\n            if not hasattr(data_state, key) or getattr(data_state, key) is None:\n                missing_state.add(key)\n\n        if missing_state:\n            hints = []\n            for k in sorted(missing_state):\n                hint = self.state_hints.get(k, \"\")\n                hints.append(f\"  - {k}\" + (f\" ({hint})\" if hint else \"\"))\n            raise ComponentValidationError(\n                f\"[Runtime] Component '{self.name}' missing required state:\\n\" + \"\\n\".join(hints)\n            )\n\n        # Check GDF columns (if component requires columns)\n        if self.requires_columns:\n            if data_state.infer_gdf is None:\n                raise ComponentValidationError(\n                    f\"[Runtime] Component '{self.name}' requires GDF columns {self.requires_columns} \"\n                    f\"but infer_gdf is None\"\n                )\n\n            available_cols = set(data_state.infer_gdf.columns)\n            missing_cols = self.requires_columns - available_cols\n\n            if missing_cols:\n                hints = []\n                for c in sorted(missing_cols):\n                    hint = self.column_hints.get(c, \"\")\n                    hints.append(f\"  - {c}\" + (f\" ({hint})\" if hint else \"\"))\n                raise ComponentValidationError(\n                    f\"[Runtime] Component '{self.name}' missing required GDF columns:\\n\" + \"\\n\".join(hints)\n                )\n\n        return method(self, data_state)\n    return wrapper\n</code></pre>"},{"location":"api/components/base/#canopyrs.engine.components.base.BaseComponent.__call__","title":"<code>__call__(data_state)</code>  <code>abstractmethod</code>","text":"<p>Run the component's core logic.</p> <p>Parameters:</p> Name Type Description Default <code>data_state</code> <code>DataState</code> <p>Current pipeline state</p> required <p>Returns:</p> Type Description <code>ComponentResult</code> <p>ComponentResult with outputs and I/O instructions</p> Source code in <code>canopyrs/engine/components/base.py</code> <pre><code>@abstractmethod\ndef __call__(self, data_state: DataState) -&gt; ComponentResult:\n    \"\"\"\n    Run the component's core logic.\n\n    Args:\n        data_state: Current pipeline state\n\n    Returns:\n        ComponentResult with outputs and I/O instructions\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/components/base/#canopyrs.engine.components.base.BaseComponent.describe","title":"<code>describe()</code>","text":"<p>Return a human-readable description of this component's requirements.</p> Source code in <code>canopyrs/engine/components/base.py</code> <pre><code>def describe(self) -&gt; str:\n    \"\"\"Return a human-readable description of this component's requirements.\"\"\"\n    lines = [\n        f\"Component: {self.name}\",\n        f\"  Requires state: {self.requires_state or 'none'}\",\n        f\"  Requires columns: {self.requires_columns or 'none'}\",\n        f\"  Produces state: {self.produces_state or 'none'}\",\n        f\"  Produces columns: {self.produces_columns or 'none'}\",\n    ]\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"api/components/base/#canopyrs.engine.components.base.BaseComponent.validate","title":"<code>validate(available_state=None, available_columns=None, raise_on_error=True)</code>","text":"<p>Validate that this component can run with the given inputs.</p> <p>Used by Pipeline before running to catch config errors early.</p> <p>Parameters:</p> Name Type Description Default <code>available_state</code> <code>Set[str]</code> <p>Set of available state keys</p> <code>None</code> <code>available_columns</code> <code>Set[str]</code> <p>Set of available GDF column names</p> <code>None</code> <code>raise_on_error</code> <code>bool</code> <p>If True, raise ComponentValidationError</p> <code>True</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of error messages (empty if valid)</p> Source code in <code>canopyrs/engine/components/base.py</code> <pre><code>def validate(\n    self,\n    available_state: Set[str] = None,\n    available_columns: Set[str] = None,\n    raise_on_error: bool = True,\n) -&gt; List[str]:\n    \"\"\"\n    Validate that this component can run with the given inputs.\n\n    Used by Pipeline before running to catch config errors early.\n\n    Args:\n        available_state: Set of available state keys\n        available_columns: Set of available GDF column names\n        raise_on_error: If True, raise ComponentValidationError\n\n    Returns:\n        List of error messages (empty if valid)\n    \"\"\"\n    errors = []\n    available_state = available_state or set()\n    available_columns = available_columns or set()\n\n    # Check state requirements\n    missing_state = self.requires_state - available_state\n    for key in missing_state:\n        hint = self.state_hints.get(key, \"\")\n        msg = f\"Missing required state: '{key}'\"\n        if hint:\n            msg += f\"\\n    -&gt; Hint: {hint}\"\n        errors.append(msg)\n\n    # Check column requirements\n    missing_columns = self.requires_columns - available_columns\n    for col in missing_columns:\n        hint = self.column_hints.get(col, \"\")\n        msg = f\"Missing required column: '{col}'\"\n        if hint:\n            msg += f\"\\n    -&gt; Hint: {hint}\"\n        errors.append(msg)\n\n    if errors and raise_on_error:\n        error_msg = f\"Component '{self.name}' validation failed:\\n\"\n        error_msg += \"\\n\".join(f\"  * {e}\" for e in errors)\n        raise ComponentValidationError(error_msg)\n\n    return errors\n</code></pre>"},{"location":"api/components/classifier/","title":"Classifier","text":"<p>               Bases: <code>BaseComponent</code></p> <p>Classifies objects in polygon-tiled imagery.</p> Requirements <ul> <li>tiles_path: Directory containing polygon tiles</li> <li>infer_coco_path: COCO annotations with instance masks</li> </ul> Produces <ul> <li>Updated infer_gdf with classification results</li> <li>Columns: classifier_score, classifier_class, classifier_scores</li> </ul> Source code in <code>canopyrs/engine/components/classifier.py</code> <pre><code>class ClassifierComponent(BaseComponent):\n    \"\"\"\n    Classifies objects in polygon-tiled imagery.\n\n    Requirements:\n        - tiles_path: Directory containing polygon tiles\n        - infer_coco_path: COCO annotations with instance masks\n\n    Produces:\n        - Updated infer_gdf with classification results\n        - Columns: classifier_score, classifier_class, classifier_scores\n    \"\"\"\n\n    name = 'classifier'\n\n    BASE_REQUIRES_STATE = {StateKey.TILES_PATH, StateKey.INFER_COCO_PATH}\n    BASE_REQUIRES_COLUMNS: Set[str] = set()\n\n    BASE_PRODUCES_STATE = {StateKey.INFER_GDF, StateKey.INFER_COCO_PATH}\n    BASE_PRODUCES_COLUMNS = {Col.CLASSIFIER_SCORE, Col.CLASSIFIER_CLASS, Col.CLASSIFIER_SCORES}\n\n    BASE_STATE_HINTS = {\n        StateKey.TILES_PATH: \"Classifier needs polygon tiles. Add a tilerizer with tile_type='polygon'.\",\n        StateKey.INFER_COCO_PATH: \"Classifier needs COCO annotations from a polygon tilerizer.\",\n    }\n\n    BASE_COLUMN_HINTS = {\n        Col.OBJECT_ID: \"Classifier needs object IDs to merge results back to infer_gdf.\",\n    }\n\n    def __init__(\n        self,\n        config: ClassifierConfig,\n        parent_output_path: str = None,\n        component_id: int = None\n    ):\n        super().__init__(config, parent_output_path, component_id)\n\n        # Store model class (instantiate in __call__ to avoid loading during validation)\n        if config.model not in CLASSIFIER_REGISTRY:\n            raise ValueError(f'Invalid classifier model: {config.model}')\n        self._model_class = CLASSIFIER_REGISTRY.get(config.model)\n\n        # Set requirements\n        self.requires_state = set(self.BASE_REQUIRES_STATE)\n        self.requires_columns = set(self.BASE_REQUIRES_COLUMNS)\n        self.produces_state = set(self.BASE_PRODUCES_STATE)\n        self.produces_columns = set(self.BASE_PRODUCES_COLUMNS)\n\n        # Set hints\n        self.state_hints = dict(self.BASE_STATE_HINTS)\n        self.column_hints = dict(self.BASE_COLUMN_HINTS)\n\n    @classmethod\n    def run_standalone(\n        cls,\n        config: ClassifierConfig,\n        tiles_path: str,\n        infer_coco_path: str,\n        output_path: str,\n    ) -&gt; 'DataState':\n        \"\"\"\n        Run classifier standalone on polygon-tiled imagery.\n\n        Args:\n            config: Classifier configuration\n            tiles_path: Path to directory containing polygon tiles\n            infer_coco_path: Path to COCO annotations with instance masks\n            output_path: Where to save outputs\n\n        Returns:\n            DataState with classification results (access .infer_gdf for the GeoDataFrame)\n\n        Example:\n            result = ClassifierComponent.run_standalone(\n                config=ClassifierConfig(model='resnet50', ...),\n                tiles_path='./polygon_tiles',\n                infer_coco_path='./coco.json',\n                output_path='./output',\n            )\n            print(result.infer_gdf)\n        \"\"\"\n        from canopyrs.engine.pipeline import run_component\n        return run_component(\n            component=cls(config),\n            output_path=output_path,\n            tiles_path=tiles_path,\n            infer_coco_path=infer_coco_path,\n        )\n\n    @validate_requirements\n    def __call__(self, data_state: DataState) -&gt; ComponentResult:\n        \"\"\"\n        Run classification on polygon tiles.\n\n        Returns flattened DataFrame (no geometry) with classification results.\n        Pipeline handles merging into existing GDF.\n        \"\"\"\n\n        classifier = self._model_class(self.config)\n\n        # Create dataset\n        infer_ds = InstanceSegmentationLabeledRasterCocoDataset(\n            root_path=[data_state.tiles_path, Path(data_state.infer_coco_path).parent],\n            transform=None,\n            fold=INFER_AOI_NAME,\n            other_attributes_names_to_pass=[Col.OBJECT_ID]\n        )\n\n        # Run inference\n        tiles_paths, class_scores, class_predictions, object_ids = classifier.infer(\n            infer_ds, collate_fn_infer_image_masks\n        )\n\n        # Flatten outputs into DataFrame (no geometry - will merge into existing)\n        df = pd.DataFrame({\n            Col.OBJECT_ID: object_ids,\n            Col.TILE_PATH: tiles_paths,  # Include for fallback merge key\n            Col.CLASSIFIER_CLASS: class_predictions,\n            Col.CLASSIFIER_SCORE: [\n                scores[pred_idx] for scores, pred_idx in zip(class_scores, class_predictions)\n            ],\n            Col.CLASSIFIER_SCORES: class_scores,\n        })\n\n        # Component-specific validation: warn about unclassified items\n        unclassified = df[Col.CLASSIFIER_CLASS].isnull().sum()\n        if unclassified &gt; 0:\n            warnings.warn(f\"{unclassified} items could not be classified.\")\n\n        print(f\"ClassifierComponent: Classified {len(df) - unclassified}/{len(df)} items.\")\n\n        return ComponentResult(\n            gdf=df,  # DataFrame, not GeoDataFrame - no geometry\n            produced_columns={Col.CLASSIFIER_SCORE, Col.CLASSIFIER_CLASS, Col.CLASSIFIER_SCORES},\n            objects_are_new=False,\n            save_gpkg=True,\n            gpkg_name_suffix=\"notaggregated\",  # classifier saves final results\n            save_coco=True,\n            coco_scores_column=Col.CLASSIFIER_SCORE,\n            coco_categories_column=Col.CLASSIFIER_CLASS,\n        )\n</code></pre>"},{"location":"api/components/classifier/#canopyrs.engine.components.classifier.ClassifierComponent.__call__","title":"<code>__call__(data_state)</code>","text":"<p>Run classification on polygon tiles.</p> <p>Returns flattened DataFrame (no geometry) with classification results. Pipeline handles merging into existing GDF.</p> Source code in <code>canopyrs/engine/components/classifier.py</code> <pre><code>@validate_requirements\ndef __call__(self, data_state: DataState) -&gt; ComponentResult:\n    \"\"\"\n    Run classification on polygon tiles.\n\n    Returns flattened DataFrame (no geometry) with classification results.\n    Pipeline handles merging into existing GDF.\n    \"\"\"\n\n    classifier = self._model_class(self.config)\n\n    # Create dataset\n    infer_ds = InstanceSegmentationLabeledRasterCocoDataset(\n        root_path=[data_state.tiles_path, Path(data_state.infer_coco_path).parent],\n        transform=None,\n        fold=INFER_AOI_NAME,\n        other_attributes_names_to_pass=[Col.OBJECT_ID]\n    )\n\n    # Run inference\n    tiles_paths, class_scores, class_predictions, object_ids = classifier.infer(\n        infer_ds, collate_fn_infer_image_masks\n    )\n\n    # Flatten outputs into DataFrame (no geometry - will merge into existing)\n    df = pd.DataFrame({\n        Col.OBJECT_ID: object_ids,\n        Col.TILE_PATH: tiles_paths,  # Include for fallback merge key\n        Col.CLASSIFIER_CLASS: class_predictions,\n        Col.CLASSIFIER_SCORE: [\n            scores[pred_idx] for scores, pred_idx in zip(class_scores, class_predictions)\n        ],\n        Col.CLASSIFIER_SCORES: class_scores,\n    })\n\n    # Component-specific validation: warn about unclassified items\n    unclassified = df[Col.CLASSIFIER_CLASS].isnull().sum()\n    if unclassified &gt; 0:\n        warnings.warn(f\"{unclassified} items could not be classified.\")\n\n    print(f\"ClassifierComponent: Classified {len(df) - unclassified}/{len(df)} items.\")\n\n    return ComponentResult(\n        gdf=df,  # DataFrame, not GeoDataFrame - no geometry\n        produced_columns={Col.CLASSIFIER_SCORE, Col.CLASSIFIER_CLASS, Col.CLASSIFIER_SCORES},\n        objects_are_new=False,\n        save_gpkg=True,\n        gpkg_name_suffix=\"notaggregated\",  # classifier saves final results\n        save_coco=True,\n        coco_scores_column=Col.CLASSIFIER_SCORE,\n        coco_categories_column=Col.CLASSIFIER_CLASS,\n    )\n</code></pre>"},{"location":"api/components/classifier/#canopyrs.engine.components.classifier.ClassifierComponent.run_standalone","title":"<code>run_standalone(config, tiles_path, infer_coco_path, output_path)</code>  <code>classmethod</code>","text":"<p>Run classifier standalone on polygon-tiled imagery.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ClassifierConfig</code> <p>Classifier configuration</p> required <code>tiles_path</code> <code>str</code> <p>Path to directory containing polygon tiles</p> required <code>infer_coco_path</code> <code>str</code> <p>Path to COCO annotations with instance masks</p> required <code>output_path</code> <code>str</code> <p>Where to save outputs</p> required <p>Returns:</p> Type Description <code>DataState</code> <p>DataState with classification results (access .infer_gdf for the GeoDataFrame)</p> Example <p>result = ClassifierComponent.run_standalone(     config=ClassifierConfig(model='resnet50', ...),     tiles_path='./polygon_tiles',     infer_coco_path='./coco.json',     output_path='./output', ) print(result.infer_gdf)</p> Source code in <code>canopyrs/engine/components/classifier.py</code> <pre><code>@classmethod\ndef run_standalone(\n    cls,\n    config: ClassifierConfig,\n    tiles_path: str,\n    infer_coco_path: str,\n    output_path: str,\n) -&gt; 'DataState':\n    \"\"\"\n    Run classifier standalone on polygon-tiled imagery.\n\n    Args:\n        config: Classifier configuration\n        tiles_path: Path to directory containing polygon tiles\n        infer_coco_path: Path to COCO annotations with instance masks\n        output_path: Where to save outputs\n\n    Returns:\n        DataState with classification results (access .infer_gdf for the GeoDataFrame)\n\n    Example:\n        result = ClassifierComponent.run_standalone(\n            config=ClassifierConfig(model='resnet50', ...),\n            tiles_path='./polygon_tiles',\n            infer_coco_path='./coco.json',\n            output_path='./output',\n        )\n        print(result.infer_gdf)\n    \"\"\"\n    from canopyrs.engine.pipeline import run_component\n    return run_component(\n        component=cls(config),\n        output_path=output_path,\n        tiles_path=tiles_path,\n        infer_coco_path=infer_coco_path,\n    )\n</code></pre>"},{"location":"api/components/detector/","title":"Detector","text":"<p>               Bases: <code>BaseComponent</code></p> <p>Runs object detection on image tiles.</p> Requirements <ul> <li>tiles_path: Directory containing tiles to process</li> </ul> Produces <ul> <li>infer_gdf: GeoDataFrame with detected bounding boxes</li> <li>Columns: geometry, object_id, tile_path, detector_score, detector_class</li> </ul> Source code in <code>canopyrs/engine/components/detector.py</code> <pre><code>class DetectorComponent(BaseComponent):\n    \"\"\"\n    Runs object detection on image tiles.\n\n    Requirements:\n        - tiles_path: Directory containing tiles to process\n\n    Produces:\n        - infer_gdf: GeoDataFrame with detected bounding boxes\n        - Columns: geometry, object_id, tile_path, detector_score, detector_class\n    \"\"\"\n\n    name = 'detector'\n\n    BASE_REQUIRES_STATE = {StateKey.TILES_PATH}\n    BASE_REQUIRES_COLUMNS: Set[str] = set()\n\n    BASE_PRODUCES_STATE = {StateKey.INFER_GDF, StateKey.INFER_COCO_PATH}\n    BASE_PRODUCES_COLUMNS = {Col.GEOMETRY, Col.OBJECT_ID, Col.TILE_PATH, Col.DETECTOR_SCORE, Col.DETECTOR_CLASS}\n\n    BASE_STATE_HINTS = {\n        StateKey.TILES_PATH: (\n            \"Detector needs tiles to process. Add a tilerizer before detector.\"\n        ),\n    }\n\n    BASE_COLUMN_HINTS: dict = {}\n\n    def __init__(\n        self,\n        config: DetectorConfig,\n        parent_output_path: str = None,\n        component_id: int = None\n    ):\n        super().__init__(config, parent_output_path, component_id)\n\n        # Store model class (instantiate in __call__ to avoid loading during validation)\n        if config.model not in DETECTOR_REGISTRY:\n            raise ValueError(f'Invalid detector model: {config.model}')\n        self._model_class = DETECTOR_REGISTRY.get(config.model)\n\n        # Set requirements\n        self.requires_state = set(self.BASE_REQUIRES_STATE)\n        self.requires_columns = set(self.BASE_REQUIRES_COLUMNS)\n        self.produces_state = set(self.BASE_PRODUCES_STATE)\n        self.produces_columns = set(self.BASE_PRODUCES_COLUMNS)\n\n        # Set hints\n        self.state_hints = dict(self.BASE_STATE_HINTS)\n        self.column_hints = dict(self.BASE_COLUMN_HINTS)\n\n    @classmethod\n    def run_standalone(\n        cls,\n        config: DetectorConfig,\n        tiles_path: str,\n        output_path: str,\n    ) -&gt; 'DataState':\n        \"\"\"\n        Run detector standalone on pre-tiled imagery.\n\n        Args:\n            config: Detector configuration\n            tiles_path: Path to directory containing tiles\n            output_path: Where to save outputs\n\n        Returns:\n            DataState with detection results (access .infer_gdf for the GeoDataFrame)\n\n        Example:\n            result = DetectorComponent.run_standalone(\n                config=DetectorConfig(model='faster_rcnn_detectron2', ...),\n                tiles_path='./tiles',\n                output_path='./output',\n            )\n            print(result.infer_gdf)\n        \"\"\"\n        from canopyrs.engine.pipeline import run_component\n        return run_component(\n            component=cls(config),\n            output_path=output_path,\n            tiles_path=tiles_path,\n        )\n\n    @validate_requirements\n    def __call__(self, data_state: DataState) -&gt; ComponentResult:\n        \"\"\"\n        Run object detection on tiles.\n\n        Returns flattened GDF. Pipeline handles merging and object_id assignment.\n        \"\"\"\n\n        detector = self._model_class(self.config)\n\n        # Create dataset from tiles\n        infer_ds = UnlabeledRasterDataset(\n            fold=None,\n            root_path=data_state.tiles_path,\n            transform=None\n        )\n\n        # Run inference\n        tiles_paths, boxes, boxes_scores, classes = detector.infer(infer_ds, collate_fn_images)\n\n        # Flatten outputs into GDF\n        rows = []\n        unique_id = 0\n        for i, tile_path in enumerate(tiles_paths):\n            for box_geom, score, cls_id in zip(boxes[i], boxes_scores[i], classes[i]):\n                rows.append({\n                    Col.GEOMETRY: box_geom,\n                    Col.TILE_PATH: str(tile_path),\n                    Col.DETECTOR_SCORE: score,\n                    Col.DETECTOR_CLASS: cls_id,\n                    Col.OBJECT_ID: unique_id,\n                })\n                unique_id += 1\n\n        # Create GDF\n        if not rows:\n            gdf = gpd.GeoDataFrame(\n                columns=list(self.produces_columns),\n                crs=None\n            )\n        else:\n            gdf = gpd.GeoDataFrame(rows, geometry=Col.GEOMETRY, crs=None)\n            # Ensure geometry is valid\n            gdf[Col.GEOMETRY] = gdf[Col.GEOMETRY].buffer(0)\n            gdf = gdf[gdf.is_valid &amp; ~gdf.is_empty]\n\n        print(f\"DetectorComponent: Generated {len(gdf)} detections.\")\n\n        return ComponentResult(\n            gdf=gdf,\n            produced_columns=self.produces_columns,\n            objects_are_new=True,\n            save_gpkg=True,\n            gpkg_name_suffix=\"notaggregated\",\n            save_coco=True,\n            coco_scores_column=Col.DETECTOR_SCORE,\n            coco_categories_column=Col.DETECTOR_CLASS,\n        )\n</code></pre>"},{"location":"api/components/detector/#canopyrs.engine.components.detector.DetectorComponent.__call__","title":"<code>__call__(data_state)</code>","text":"<p>Run object detection on tiles.</p> <p>Returns flattened GDF. Pipeline handles merging and object_id assignment.</p> Source code in <code>canopyrs/engine/components/detector.py</code> <pre><code>@validate_requirements\ndef __call__(self, data_state: DataState) -&gt; ComponentResult:\n    \"\"\"\n    Run object detection on tiles.\n\n    Returns flattened GDF. Pipeline handles merging and object_id assignment.\n    \"\"\"\n\n    detector = self._model_class(self.config)\n\n    # Create dataset from tiles\n    infer_ds = UnlabeledRasterDataset(\n        fold=None,\n        root_path=data_state.tiles_path,\n        transform=None\n    )\n\n    # Run inference\n    tiles_paths, boxes, boxes_scores, classes = detector.infer(infer_ds, collate_fn_images)\n\n    # Flatten outputs into GDF\n    rows = []\n    unique_id = 0\n    for i, tile_path in enumerate(tiles_paths):\n        for box_geom, score, cls_id in zip(boxes[i], boxes_scores[i], classes[i]):\n            rows.append({\n                Col.GEOMETRY: box_geom,\n                Col.TILE_PATH: str(tile_path),\n                Col.DETECTOR_SCORE: score,\n                Col.DETECTOR_CLASS: cls_id,\n                Col.OBJECT_ID: unique_id,\n            })\n            unique_id += 1\n\n    # Create GDF\n    if not rows:\n        gdf = gpd.GeoDataFrame(\n            columns=list(self.produces_columns),\n            crs=None\n        )\n    else:\n        gdf = gpd.GeoDataFrame(rows, geometry=Col.GEOMETRY, crs=None)\n        # Ensure geometry is valid\n        gdf[Col.GEOMETRY] = gdf[Col.GEOMETRY].buffer(0)\n        gdf = gdf[gdf.is_valid &amp; ~gdf.is_empty]\n\n    print(f\"DetectorComponent: Generated {len(gdf)} detections.\")\n\n    return ComponentResult(\n        gdf=gdf,\n        produced_columns=self.produces_columns,\n        objects_are_new=True,\n        save_gpkg=True,\n        gpkg_name_suffix=\"notaggregated\",\n        save_coco=True,\n        coco_scores_column=Col.DETECTOR_SCORE,\n        coco_categories_column=Col.DETECTOR_CLASS,\n    )\n</code></pre>"},{"location":"api/components/detector/#canopyrs.engine.components.detector.DetectorComponent.run_standalone","title":"<code>run_standalone(config, tiles_path, output_path)</code>  <code>classmethod</code>","text":"<p>Run detector standalone on pre-tiled imagery.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>DetectorConfig</code> <p>Detector configuration</p> required <code>tiles_path</code> <code>str</code> <p>Path to directory containing tiles</p> required <code>output_path</code> <code>str</code> <p>Where to save outputs</p> required <p>Returns:</p> Type Description <code>DataState</code> <p>DataState with detection results (access .infer_gdf for the GeoDataFrame)</p> Example <p>result = DetectorComponent.run_standalone(     config=DetectorConfig(model='faster_rcnn_detectron2', ...),     tiles_path='./tiles',     output_path='./output', ) print(result.infer_gdf)</p> Source code in <code>canopyrs/engine/components/detector.py</code> <pre><code>@classmethod\ndef run_standalone(\n    cls,\n    config: DetectorConfig,\n    tiles_path: str,\n    output_path: str,\n) -&gt; 'DataState':\n    \"\"\"\n    Run detector standalone on pre-tiled imagery.\n\n    Args:\n        config: Detector configuration\n        tiles_path: Path to directory containing tiles\n        output_path: Where to save outputs\n\n    Returns:\n        DataState with detection results (access .infer_gdf for the GeoDataFrame)\n\n    Example:\n        result = DetectorComponent.run_standalone(\n            config=DetectorConfig(model='faster_rcnn_detectron2', ...),\n            tiles_path='./tiles',\n            output_path='./output',\n        )\n        print(result.infer_gdf)\n    \"\"\"\n    from canopyrs.engine.pipeline import run_component\n    return run_component(\n        component=cls(config),\n        output_path=output_path,\n        tiles_path=tiles_path,\n    )\n</code></pre>"},{"location":"api/components/segmenter/","title":"Segmenter","text":"<p>               Bases: <code>BaseComponent</code></p> <p>Runs instance segmentation on image tiles.</p> Modes <ul> <li>With box prompts: Requires infer_coco_path with detection boxes</li> <li>Without box prompts: Runs segmentation on full tiles</li> </ul> Requirements <ul> <li>tiles_path: Directory containing tiles</li> <li>infer_coco_path + infer_gdf: If model requires box prompts</li> </ul> Produces <ul> <li>infer_gdf: GeoDataFrame with segmented polygons</li> <li>Columns: segmenter_score (+ preserves detector columns if present)</li> </ul> Source code in <code>canopyrs/engine/components/segmenter.py</code> <pre><code>class SegmenterComponent(BaseComponent):\n    \"\"\"\n    Runs instance segmentation on image tiles.\n\n    Modes:\n        - With box prompts: Requires infer_coco_path with detection boxes\n        - Without box prompts: Runs segmentation on full tiles\n\n    Requirements:\n        - tiles_path: Directory containing tiles\n        - infer_coco_path + infer_gdf: If model requires box prompts\n\n    Produces:\n        - infer_gdf: GeoDataFrame with segmented polygons\n        - Columns: segmenter_score (+ preserves detector columns if present)\n    \"\"\"\n\n    name = 'segmenter'\n\n    BASE_REQUIRES_STATE = {StateKey.TILES_PATH}\n    BASE_REQUIRES_COLUMNS: Set[str] = set()\n\n    BASE_PRODUCES_STATE = {StateKey.INFER_GDF, StateKey.INFER_COCO_PATH}\n    BASE_PRODUCES_COLUMNS = {Col.GEOMETRY, Col.OBJECT_ID, Col.TILE_PATH, Col.SEGMENTER_SCORE}\n\n    BASE_STATE_HINTS = {\n        StateKey.TILES_PATH: \"Segmenter needs tiles to process. Add a tilerizer before segmenter.\",\n        StateKey.INFER_COCO_PATH: \"This segmenter model requires box prompts from a COCO file.\",\n        StateKey.INFER_GDF: \"This segmenter model requires a GeoDataFrame with detection boxes.\",\n    }\n\n    BASE_COLUMN_HINTS = {\n        Col.OBJECT_ID: \"Segmenter needs object IDs to associate masks with detections.\",\n    }\n\n    def __init__(\n        self,\n        config: SegmenterConfig,\n        parent_output_path: str = None,\n        component_id: int = None\n    ):\n        super().__init__(config, parent_output_path, component_id)\n\n        # Get model class (without instantiating) to check requirements\n        if config.model not in SEGMENTER_REGISTRY:\n            raise ValueError(f'Invalid segmenter model: {config.model}')\n        self._model_class = SEGMENTER_REGISTRY.get(config.model)\n\n        # Set base requirements\n        self.requires_state = set(self.BASE_REQUIRES_STATE)\n        self.requires_columns = set(self.BASE_REQUIRES_COLUMNS)\n        self.produces_state = set(self.BASE_PRODUCES_STATE)\n        self.produces_columns = set(self.BASE_PRODUCES_COLUMNS)\n\n        # Set hints\n        self.state_hints = dict(self.BASE_STATE_HINTS)\n        self.column_hints = dict(self.BASE_COLUMN_HINTS)\n\n        # Add model-specific requirements\n        if self._model_class.REQUIRES_BOX_PROMPT:\n            self.requires_state.add(StateKey.INFER_COCO_PATH)\n            self.state_hints[StateKey.INFER_COCO_PATH] = (\n                f\"The '{config.model}' segmenter requires box prompts. \"\n                f\"Add a detector before segmenter.\"\n            )\n\n    @classmethod\n    def run_standalone(\n        cls,\n        config: SegmenterConfig,\n        tiles_path: str,\n        output_path: str,\n        infer_coco_path: str = None,\n    ) -&gt; 'DataState':\n        \"\"\"\n        Run segmenter standalone on pre-tiled imagery.\n\n        Args:\n            config: Segmenter configuration\n            tiles_path: Path to directory containing tiles\n            output_path: Where to save outputs\n            infer_coco_path: Path to COCO file with detection boxes\n                             (required if the model needs box prompts, like SAM)\n\n        Returns:\n            DataState with segmentation results (access .infer_gdf for the GeoDataFrame)\n\n        Example:\n            result = SegmenterComponent.run_standalone(\n                config=SegmenterConfig(model='sam2', ...),\n                tiles_path='./tiles',\n                output_path='./output',\n            )\n            print(result.infer_gdf)\n        \"\"\"\n        from canopyrs.engine.pipeline import run_component\n        return run_component(\n            component=cls(config),\n            output_path=output_path,\n            tiles_path=tiles_path,\n            infer_coco_path=infer_coco_path,\n        )\n\n    @validate_requirements\n    def __call__(self, data_state: DataState) -&gt; ComponentResult:\n        \"\"\"\n        Run instance segmentation on tiles.\n\n        Returns flattened GDF with new geometries (masks).\n        Pipeline handles merging with existing GDF (to preserve detector columns).\n        \"\"\"\n\n        segmenter = self._model_class(self.config)\n\n        # Create appropriate dataset\n        data_paths = [data_state.tiles_path]\n\n        if segmenter.REQUIRES_BOX_PROMPT:\n            data_paths.append(Path(data_state.infer_coco_path).parent)\n            dataset = DetectionLabeledRasterCocoDataset(\n                fold=INFER_AOI_NAME,\n                root_path=data_paths,\n                box_padding_percentage=self.config.box_padding_percentage,\n                transform=None,\n                other_attributes_names_to_pass=[Col.OBJECT_ID]\n            )\n        else:\n            dataset = UnlabeledRasterDataset(\n                fold=None,\n                root_path=data_paths,\n                transform=None\n            )\n\n        # Run inference\n        tiles_paths, tiles_masks_objects_ids, tiles_masks_polygons, tiles_masks_scores = \\\n            segmenter.infer_on_dataset(dataset)\n\n        # Flatten outputs into GDF\n        rows = []\n        unique_id = 0\n        for i in range(len(tiles_paths)):\n            for j in range(len(tiles_masks_polygons[i])):\n                row = {\n                    Col.TILE_PATH: tiles_paths[i],\n                    Col.GEOMETRY: tiles_masks_polygons[i][j],\n                    Col.SEGMENTER_SCORE: tiles_masks_scores[i][j],\n                }\n                # Include object_id if available (from detector), or assign new unique id otherwise\n                if tiles_masks_objects_ids is not None:\n                    row[Col.OBJECT_ID] = tiles_masks_objects_ids[i][j]\n                else:\n                    row[Col.OBJECT_ID] = unique_id\n                    unique_id += 1\n                rows.append(row)\n\n        # Create GDF with new geometries (masks)\n        gdf = gpd.GeoDataFrame(rows, geometry=Col.GEOMETRY, crs=None) if rows else gpd.GeoDataFrame(\n            columns=self.produces_columns,\n            crs=None\n        )\n\n        print(f\"SegmenterComponent: Generated {len(gdf)} masks.\")\n\n        return ComponentResult(\n            gdf=gdf,\n            produced_columns=self.produces_columns,\n            objects_are_new=not self._model_class.REQUIRES_BOX_PROMPT,\n            save_gpkg=True,\n            gpkg_name_suffix=\"notaggregated\",\n            save_coco=True,\n            coco_scores_column=Col.SEGMENTER_SCORE,\n            coco_categories_column=None,\n        )\n</code></pre>"},{"location":"api/components/segmenter/#canopyrs.engine.components.segmenter.SegmenterComponent.__call__","title":"<code>__call__(data_state)</code>","text":"<p>Run instance segmentation on tiles.</p> <p>Returns flattened GDF with new geometries (masks). Pipeline handles merging with existing GDF (to preserve detector columns).</p> Source code in <code>canopyrs/engine/components/segmenter.py</code> <pre><code>@validate_requirements\ndef __call__(self, data_state: DataState) -&gt; ComponentResult:\n    \"\"\"\n    Run instance segmentation on tiles.\n\n    Returns flattened GDF with new geometries (masks).\n    Pipeline handles merging with existing GDF (to preserve detector columns).\n    \"\"\"\n\n    segmenter = self._model_class(self.config)\n\n    # Create appropriate dataset\n    data_paths = [data_state.tiles_path]\n\n    if segmenter.REQUIRES_BOX_PROMPT:\n        data_paths.append(Path(data_state.infer_coco_path).parent)\n        dataset = DetectionLabeledRasterCocoDataset(\n            fold=INFER_AOI_NAME,\n            root_path=data_paths,\n            box_padding_percentage=self.config.box_padding_percentage,\n            transform=None,\n            other_attributes_names_to_pass=[Col.OBJECT_ID]\n        )\n    else:\n        dataset = UnlabeledRasterDataset(\n            fold=None,\n            root_path=data_paths,\n            transform=None\n        )\n\n    # Run inference\n    tiles_paths, tiles_masks_objects_ids, tiles_masks_polygons, tiles_masks_scores = \\\n        segmenter.infer_on_dataset(dataset)\n\n    # Flatten outputs into GDF\n    rows = []\n    unique_id = 0\n    for i in range(len(tiles_paths)):\n        for j in range(len(tiles_masks_polygons[i])):\n            row = {\n                Col.TILE_PATH: tiles_paths[i],\n                Col.GEOMETRY: tiles_masks_polygons[i][j],\n                Col.SEGMENTER_SCORE: tiles_masks_scores[i][j],\n            }\n            # Include object_id if available (from detector), or assign new unique id otherwise\n            if tiles_masks_objects_ids is not None:\n                row[Col.OBJECT_ID] = tiles_masks_objects_ids[i][j]\n            else:\n                row[Col.OBJECT_ID] = unique_id\n                unique_id += 1\n            rows.append(row)\n\n    # Create GDF with new geometries (masks)\n    gdf = gpd.GeoDataFrame(rows, geometry=Col.GEOMETRY, crs=None) if rows else gpd.GeoDataFrame(\n        columns=self.produces_columns,\n        crs=None\n    )\n\n    print(f\"SegmenterComponent: Generated {len(gdf)} masks.\")\n\n    return ComponentResult(\n        gdf=gdf,\n        produced_columns=self.produces_columns,\n        objects_are_new=not self._model_class.REQUIRES_BOX_PROMPT,\n        save_gpkg=True,\n        gpkg_name_suffix=\"notaggregated\",\n        save_coco=True,\n        coco_scores_column=Col.SEGMENTER_SCORE,\n        coco_categories_column=None,\n    )\n</code></pre>"},{"location":"api/components/segmenter/#canopyrs.engine.components.segmenter.SegmenterComponent.run_standalone","title":"<code>run_standalone(config, tiles_path, output_path, infer_coco_path=None)</code>  <code>classmethod</code>","text":"<p>Run segmenter standalone on pre-tiled imagery.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>SegmenterConfig</code> <p>Segmenter configuration</p> required <code>tiles_path</code> <code>str</code> <p>Path to directory containing tiles</p> required <code>output_path</code> <code>str</code> <p>Where to save outputs</p> required <code>infer_coco_path</code> <code>str</code> <p>Path to COCO file with detection boxes              (required if the model needs box prompts, like SAM)</p> <code>None</code> <p>Returns:</p> Type Description <code>DataState</code> <p>DataState with segmentation results (access .infer_gdf for the GeoDataFrame)</p> Example <p>result = SegmenterComponent.run_standalone(     config=SegmenterConfig(model='sam2', ...),     tiles_path='./tiles',     output_path='./output', ) print(result.infer_gdf)</p> Source code in <code>canopyrs/engine/components/segmenter.py</code> <pre><code>@classmethod\ndef run_standalone(\n    cls,\n    config: SegmenterConfig,\n    tiles_path: str,\n    output_path: str,\n    infer_coco_path: str = None,\n) -&gt; 'DataState':\n    \"\"\"\n    Run segmenter standalone on pre-tiled imagery.\n\n    Args:\n        config: Segmenter configuration\n        tiles_path: Path to directory containing tiles\n        output_path: Where to save outputs\n        infer_coco_path: Path to COCO file with detection boxes\n                         (required if the model needs box prompts, like SAM)\n\n    Returns:\n        DataState with segmentation results (access .infer_gdf for the GeoDataFrame)\n\n    Example:\n        result = SegmenterComponent.run_standalone(\n            config=SegmenterConfig(model='sam2', ...),\n            tiles_path='./tiles',\n            output_path='./output',\n        )\n        print(result.infer_gdf)\n    \"\"\"\n    from canopyrs.engine.pipeline import run_component\n    return run_component(\n        component=cls(config),\n        output_path=output_path,\n        tiles_path=tiles_path,\n        infer_coco_path=infer_coco_path,\n    )\n</code></pre>"},{"location":"api/components/tilerizer/","title":"Tilerizer","text":"<p>               Bases: <code>BaseComponent</code></p> <p>Creates image tiles from raster imagery.</p> Tile types <ul> <li>'tile': Unlabeled regular grid tiles (for inference)</li> <li>'tile_labeled': Labeled regular grid tiles (for training data)</li> <li>'polygon': Per-polygon tiles (for classifier input)</li> </ul> Requirements vary by tile_type <ul> <li>'tile': imagery_path only</li> <li>'tile_labeled': imagery_path + infer_gdf</li> <li>'polygon': imagery_path + infer_gdf</li> </ul> Produces <ul> <li>tiles_path  (always)</li> <li>infer_coco_path (only for 'tile_labeled' and 'polygon')</li> </ul> Source code in <code>canopyrs/engine/components/tilerizer.py</code> <pre><code>class TilerizerComponent(BaseComponent):\n    \"\"\"\n    Creates image tiles from raster imagery.\n\n    Tile types:\n        - 'tile': Unlabeled regular grid tiles (for inference)\n        - 'tile_labeled': Labeled regular grid tiles (for training data)\n        - 'polygon': Per-polygon tiles (for classifier input)\n\n    Requirements vary by tile_type:\n        - 'tile': imagery_path only\n        - 'tile_labeled': imagery_path + infer_gdf\n        - 'polygon': imagery_path + infer_gdf\n\n    Produces:\n        - tiles_path  (always)\n        - infer_coco_path (only for 'tile_labeled' and 'polygon')\n    \"\"\"\n\n    name = 'tilerizer'\n\n    BASE_REQUIRES_STATE = {StateKey.IMAGERY_PATH}\n    BASE_REQUIRES_COLUMNS: Set[str] = set()\n\n    BASE_PRODUCES_STATE = {StateKey.TILES_PATH}\n    BASE_PRODUCES_COLUMNS: Set[str] = set()\n\n    BASE_STATE_HINTS = {\n        StateKey.IMAGERY_PATH: \"Tilerizer needs an imagery_path to the raster file.\",\n        StateKey.INFER_GDF: \"This tile_type requires a GeoDataFrame with labels/polygons.\",\n    }\n\n    BASE_COLUMN_HINTS = {\n        Col.GEOMETRY: \"GeoDataFrame must have a 'geometry' column.\",\n    }\n\n    def __init__(\n        self,\n        config: TilerizerConfig,\n        parent_output_path: str = None,\n        component_id: int = None,\n        infer_aois_config: Optional[AOIConfig] = None\n    ):\n        super().__init__(config, parent_output_path, component_id)\n        self.infer_aois_config = infer_aois_config\n\n        # Validate tile_type\n        if config.tile_type not in ['tile', 'tile_labeled', 'polygon']:\n            raise ValueError(\n                f\"Invalid tile_type: '{config.tile_type}'. \"\n                f\"Must be 'tile', 'tile_labeled', or 'polygon'.\"\n            )\n\n        # Set base requirements\n        self.requires_state = set(self.BASE_REQUIRES_STATE)\n        self.requires_columns = set(self.BASE_REQUIRES_COLUMNS)\n        self.produces_state = set(self.BASE_PRODUCES_STATE)\n        self.produces_columns = set(self.BASE_PRODUCES_COLUMNS)\n\n        # Set hints\n        self.state_hints = dict(self.BASE_STATE_HINTS)\n        self.column_hints = dict(self.BASE_COLUMN_HINTS)\n\n        # Tile-type-specific requirements\n        if config.tile_type == 'tile':\n            # Unlabeled tiles - no additional requirements or produces\n            pass\n\n        elif config.tile_type == 'tile_labeled':\n            # Labeled tiles - requires infer_gdf, produces COCO\n            self.requires_state.add(StateKey.INFER_GDF)\n            self.requires_columns.add(Col.GEOMETRY)\n            self.produces_state.add(StateKey.INFER_COCO_PATH)\n            self.state_hints[StateKey.INFER_GDF] = (\n                f\"tile_type='tile_labeled' requires infer_gdf with labels. \"\n                f\"Use tile_type='tile' for unlabeled tiles.\"\n            )\n\n        elif config.tile_type == 'polygon':\n            # Polygon tiles - requires infer_gdf, produces COCO\n            self.requires_state.add(StateKey.INFER_GDF)\n            self.requires_columns.add(Col.GEOMETRY)\n            self.produces_state.add(StateKey.INFER_COCO_PATH)\n            self.state_hints[StateKey.INFER_GDF] = (\n                f\"tile_type='polygon' requires infer_gdf with polygons.\"\n            )\n\n    @classmethod\n    def run_standalone(\n        cls,\n        config: TilerizerConfig,\n        imagery_path: str,\n        output_path: str,\n        infer_gdf: gpd.GeoDataFrame = None,\n        infer_aois_config: Optional[AOIConfig] = None,\n    ) -&gt; 'DataState':\n        \"\"\"\n        Run tilerizer standalone on raster imagery.\n\n        Args:\n            config: Tilerizer configuration (tile_type determines requirements)\n            imagery_path: Path to the raster file\n            output_path: Where to save outputs\n            infer_gdf: GeoDataFrame with labels/polygons\n                        (required for tile_type='tile_labeled' or 'polygon')\n            infer_aois_config: Area of Interest configuration (optional)\n\n        Returns:\n            DataState with tiling results (access .tiles_path for tile directory)\n\n        Example:\n            result = TilerizerComponent.run_standalone(\n                config=TilerizerConfig(tile_type='tile', tile_size=512, ...),\n                imagery_path='./raster.tif',\n                output_path='./output',\n            )\n            print(result.tiles_path)\n        \"\"\"\n        from canopyrs.engine.pipeline import run_component\n        return run_component(\n            component=cls(config, infer_aois_config=infer_aois_config),\n            output_path=output_path,\n            imagery_path=imagery_path,\n            infer_gdf=infer_gdf,\n        )\n\n    @validate_requirements\n    def __call__(self, data_state: DataState) -&gt; ComponentResult:\n        \"\"\"\n        Create tiles from raster imagery.\n\n        Returns ComponentResult - Pipeline handles state updates.\n        Tilerizer handles its own file I/O internally via geodataset.\n        \"\"\"\n        self._check_crs_match(data_state)\n\n        # Handle config columns\n        columns_to_pass = data_state.infer_gdf_columns_to_pass.copy()\n        if self.config.other_labels_attributes_column_names:\n            columns_to_pass.update(self.config.other_labels_attributes_column_names)\n\n        columns_to_pass = [col for col in columns_to_pass if col not in {Col.GEOMETRY, Col.TILE_PATH}]  # already taken care of by COCO format\n\n        # Process based on tile_type\n        if self.config.tile_type == 'tile':\n            if data_state.infer_gdf is not None:\n                raise ValueError(\n                    \"infer_gdf provided but tile_type='tile' creates unlabeled tiles. \"\n                    \"Use tile_type='tile_labeled' or 'polygon' if labels are needed for subsequent components, like a prompted Segmenter.\"\n                )\n            # Unlabeled tiles only\n            tiles_path, infer_coco_path = self._process_unlabeled_tiles(data_state)\n\n        elif self.config.tile_type == 'tile_labeled':\n            # Labeled regular grid tiles\n            tiles_path, infer_coco_path = self._process_labeled_tiles(\n                data_state, columns_to_pass\n            )\n\n        elif self.config.tile_type == 'polygon':\n            # Polygon tiles\n            tiles_path, infer_coco_path = self._process_polygon_tiles(\n                data_state, columns_to_pass\n            )\n\n        else:\n            raise ValueError(f\"Invalid tile_type: {self.config.tile_type}\")\n\n        # Save config\n        if self.output_path:\n            self.config.to_yaml(self.output_path / \"tilerizer_config.yaml\")\n\n        # Register the COCO file that geodataset already wrote (if any)\n        output_files = {}\n        if infer_coco_path is not None:\n            output_files['coco'] = infer_coco_path\n\n        return ComponentResult(\n            gdf=None,  # Tilerizer doesn't modify the GDF\n            produced_columns=columns_to_pass,\n            objects_are_new=False,\n            state_updates={\n                StateKey.TILES_PATH: tiles_path,\n                StateKey.INFER_COCO_PATH: infer_coco_path,\n            },\n            save_gpkg=False,\n            save_coco=False,  # COCO handled internally by tilerizer\n            output_files=output_files,\n        )\n\n    def _process_labeled_tiles(self, data_state: DataState, columns_to_pass: Set[str]):\n        \"\"\"Process labeled regular grid tiles (tile_type='tile_labeled').\"\"\"\n        tilerizer = LabeledRasterTilerizer(\n            raster_path=data_state.imagery_path,\n            labels_path=None,\n            labels_gdf=data_state.infer_gdf,\n            output_path=self.output_path,\n            tile_size=self.config.tile_size,\n            tile_overlap=self.config.tile_overlap,\n            aois_config=self.infer_aois_config,\n            scale_factor=self.config.scale_factor,\n            ground_resolution=self.config.ground_resolution,\n            ignore_black_white_alpha_tiles_threshold=self.config.ignore_black_white_alpha_tiles_threshold,\n            min_intersection_ratio=self.config.min_intersection_ratio,\n            ignore_tiles_without_labels=self.config.ignore_tiles_without_labels,\n            main_label_category_column_name=self.config.main_label_category_column_name,\n            other_labels_attributes_column_names=list(columns_to_pass),\n        )\n        coco_paths = tilerizer.generate_coco_dataset()\n        return tilerizer.tiles_path, coco_paths.get(INFER_AOI_NAME)\n\n    def _process_unlabeled_tiles(self, data_state: DataState):\n        \"\"\"Process unlabeled tiles (tile_type='tile' without infer_gdf).\"\"\"\n        tilerizer = RasterTilerizer(\n            raster_path=data_state.imagery_path,\n            output_path=self.output_path,\n            tile_size=self.config.tile_size,\n            tile_overlap=self.config.tile_overlap,\n            aois_config=self.infer_aois_config,\n            scale_factor=self.config.scale_factor,\n            ground_resolution=self.config.ground_resolution,\n            ignore_black_white_alpha_tiles_threshold=self.config.ignore_black_white_alpha_tiles_threshold,\n        )\n        tilerizer.generate_tiles()\n        return tilerizer.tiles_path, None\n\n    def _process_polygon_tiles(self, data_state: DataState, columns_to_pass: Set[str]):\n        \"\"\"Process polygon tiles (tile_type='polygon').\"\"\"\n        tilerizer = RasterPolygonTilerizer(\n            raster_path=data_state.imagery_path,\n            output_path=self.output_path,\n            labels_path=None,\n            labels_gdf=data_state.infer_gdf,\n            tile_size=self.config.tile_size,\n            use_variable_tile_size=self.config.use_variable_tile_size,\n            variable_tile_size_pixel_buffer=self.config.variable_tile_size_pixel_buffer,\n            aois_config=self.infer_aois_config,\n            scale_factor=self.config.scale_factor,\n            ground_resolution=self.config.ground_resolution,\n            main_label_category_column_name=self.config.main_label_category_column_name,\n            other_labels_attributes_column_names=list(columns_to_pass),\n            coco_n_workers=self.config.coco_n_workers,\n        )\n        coco_paths = tilerizer.generate_coco_dataset()\n        return tilerizer.tiles_folder_path, coco_paths.get(INFER_AOI_NAME)\n\n    def _check_crs_match(self, data_state: DataState):\n        \"\"\"Check if the CRS of the raster and GeoDataFrame match.\"\"\"\n        if data_state.infer_gdf is None:\n            return\n\n        try:\n            with rasterio.open(data_state.imagery_path) as src:\n                raster_crs = src.crs\n        except Exception as e:\n            raise RuntimeError(f\"Failed to open raster: {e}\")\n\n        gdf_crs = data_state.infer_gdf.crs\n\n        if raster_crs is not None and gdf_crs is None:\n            raise ValueError(\"Raster has CRS but infer_gdf does not.\")\n        elif raster_crs is None and gdf_crs is not None:\n            raise ValueError(\"Raster has no CRS but infer_gdf does.\")\n</code></pre>"},{"location":"api/components/tilerizer/#canopyrs.engine.components.tilerizer.TilerizerComponent.__call__","title":"<code>__call__(data_state)</code>","text":"<p>Create tiles from raster imagery.</p> <p>Returns ComponentResult - Pipeline handles state updates. Tilerizer handles its own file I/O internally via geodataset.</p> Source code in <code>canopyrs/engine/components/tilerizer.py</code> <pre><code>@validate_requirements\ndef __call__(self, data_state: DataState) -&gt; ComponentResult:\n    \"\"\"\n    Create tiles from raster imagery.\n\n    Returns ComponentResult - Pipeline handles state updates.\n    Tilerizer handles its own file I/O internally via geodataset.\n    \"\"\"\n    self._check_crs_match(data_state)\n\n    # Handle config columns\n    columns_to_pass = data_state.infer_gdf_columns_to_pass.copy()\n    if self.config.other_labels_attributes_column_names:\n        columns_to_pass.update(self.config.other_labels_attributes_column_names)\n\n    columns_to_pass = [col for col in columns_to_pass if col not in {Col.GEOMETRY, Col.TILE_PATH}]  # already taken care of by COCO format\n\n    # Process based on tile_type\n    if self.config.tile_type == 'tile':\n        if data_state.infer_gdf is not None:\n            raise ValueError(\n                \"infer_gdf provided but tile_type='tile' creates unlabeled tiles. \"\n                \"Use tile_type='tile_labeled' or 'polygon' if labels are needed for subsequent components, like a prompted Segmenter.\"\n            )\n        # Unlabeled tiles only\n        tiles_path, infer_coco_path = self._process_unlabeled_tiles(data_state)\n\n    elif self.config.tile_type == 'tile_labeled':\n        # Labeled regular grid tiles\n        tiles_path, infer_coco_path = self._process_labeled_tiles(\n            data_state, columns_to_pass\n        )\n\n    elif self.config.tile_type == 'polygon':\n        # Polygon tiles\n        tiles_path, infer_coco_path = self._process_polygon_tiles(\n            data_state, columns_to_pass\n        )\n\n    else:\n        raise ValueError(f\"Invalid tile_type: {self.config.tile_type}\")\n\n    # Save config\n    if self.output_path:\n        self.config.to_yaml(self.output_path / \"tilerizer_config.yaml\")\n\n    # Register the COCO file that geodataset already wrote (if any)\n    output_files = {}\n    if infer_coco_path is not None:\n        output_files['coco'] = infer_coco_path\n\n    return ComponentResult(\n        gdf=None,  # Tilerizer doesn't modify the GDF\n        produced_columns=columns_to_pass,\n        objects_are_new=False,\n        state_updates={\n            StateKey.TILES_PATH: tiles_path,\n            StateKey.INFER_COCO_PATH: infer_coco_path,\n        },\n        save_gpkg=False,\n        save_coco=False,  # COCO handled internally by tilerizer\n        output_files=output_files,\n    )\n</code></pre>"},{"location":"api/components/tilerizer/#canopyrs.engine.components.tilerizer.TilerizerComponent.run_standalone","title":"<code>run_standalone(config, imagery_path, output_path, infer_gdf=None, infer_aois_config=None)</code>  <code>classmethod</code>","text":"<p>Run tilerizer standalone on raster imagery.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>TilerizerConfig</code> <p>Tilerizer configuration (tile_type determines requirements)</p> required <code>imagery_path</code> <code>str</code> <p>Path to the raster file</p> required <code>output_path</code> <code>str</code> <p>Where to save outputs</p> required <code>infer_gdf</code> <code>GeoDataFrame</code> <p>GeoDataFrame with labels/polygons         (required for tile_type='tile_labeled' or 'polygon')</p> <code>None</code> <code>infer_aois_config</code> <code>Optional[AOIConfig]</code> <p>Area of Interest configuration (optional)</p> <code>None</code> <p>Returns:</p> Type Description <code>DataState</code> <p>DataState with tiling results (access .tiles_path for tile directory)</p> Example <p>result = TilerizerComponent.run_standalone(     config=TilerizerConfig(tile_type='tile', tile_size=512, ...),     imagery_path='./raster.tif',     output_path='./output', ) print(result.tiles_path)</p> Source code in <code>canopyrs/engine/components/tilerizer.py</code> <pre><code>@classmethod\ndef run_standalone(\n    cls,\n    config: TilerizerConfig,\n    imagery_path: str,\n    output_path: str,\n    infer_gdf: gpd.GeoDataFrame = None,\n    infer_aois_config: Optional[AOIConfig] = None,\n) -&gt; 'DataState':\n    \"\"\"\n    Run tilerizer standalone on raster imagery.\n\n    Args:\n        config: Tilerizer configuration (tile_type determines requirements)\n        imagery_path: Path to the raster file\n        output_path: Where to save outputs\n        infer_gdf: GeoDataFrame with labels/polygons\n                    (required for tile_type='tile_labeled' or 'polygon')\n        infer_aois_config: Area of Interest configuration (optional)\n\n    Returns:\n        DataState with tiling results (access .tiles_path for tile directory)\n\n    Example:\n        result = TilerizerComponent.run_standalone(\n            config=TilerizerConfig(tile_type='tile', tile_size=512, ...),\n            imagery_path='./raster.tif',\n            output_path='./output',\n        )\n        print(result.tiles_path)\n    \"\"\"\n    from canopyrs.engine.pipeline import run_component\n    return run_component(\n        component=cls(config, infer_aois_config=infer_aois_config),\n        output_path=output_path,\n        imagery_path=imagery_path,\n        infer_gdf=infer_gdf,\n    )\n</code></pre>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>OS: Linux (Ubuntu 22.04 recommended). Windows 10 is also supported but might be trickier to setup. MacOS is untested.</li> <li>Python: 3.10</li> <li>CUDA: 12.6 \u2014 You can install CUDA by following the NVIDIA CUDA installation guide, or directly downloading it from this link. The command <code>nvcc --version</code> should show version 12.6.</li> </ul>"},{"location":"getting-started/installation/#step-by-step","title":"Step-by-step","text":"<p>1. Clone the repository</p> <pre><code>git clone https://github.com/hugobaudchon/CanopyRS.git\ncd CanopyRS\n</code></pre> <p>2. Create a conda environment with mamba</p> <pre><code>conda create -n canopyrs -c conda-forge python=3.10 mamba\nconda activate canopyrs\n</code></pre> <p>3. Install GDAL via mamba</p> <pre><code>mamba install gdal=3.6.2 -c conda-forge\n</code></pre> <p>4. Install PyTorch with CUDA 12.6 support</p> <pre><code>pip install torch==2.7.1 torchvision==0.22.1 --index-url https://download.pytorch.org/whl/cu126\n</code></pre> <p>5. Initialize submodules</p> <pre><code>git submodule update --init --recursive\n</code></pre> <p>6. Install CanopyRS and dependencies</p> <pre><code>python -m pip install -e .\npython -m pip install --no-build-isolation -e ./detrex/detectron2 -e ./detrex\n</code></pre>"},{"location":"getting-started/installation/#sam-3-hugging-face-access-request","title":"SAM 3 \u2014 Hugging Face access request","text":"<p>SAM 3 is a gated model hosted by Meta on Hugging Face. If you plan to use SAM 3 (either directly or through a SAM 3 preset), you must request access before your first run:</p> <ol> <li>Go to the facebook/sam3 model page on Hugging Face.</li> <li>Click \"Request access\" and accept Meta's license terms.</li> <li>Make sure you are logged in to Hugging Face on your machine:    <pre><code>huggingface-cli login\n</code></pre></li> </ol> <p>Without this step, any pipeline using SAM 3 will fail when trying to download the base model weights.</p>"},{"location":"getting-started/installation/#known-issues","title":"Known issues","text":"<p>You will likely encounter this error during installation:</p> <pre><code>sam2 0.4.1 requires iopath&gt;=0.1.10, but you have iopath 0.1.9 which is incompatible\n</code></pre> <p>This is a conflict between Detectron2 and SAM2 libraries, but it can be ignored and should not impact installation or usage of the pipeline.</p>"},{"location":"getting-started/installation/#verify-the-installation","title":"Verify the installation","text":"<pre><code>python -c \"import canopyrs; print('CanopyRS installed successfully')\"\n</code></pre>"},{"location":"getting-started/quickstart/","title":"Quickstart","text":"<p>Run tree detection on a single orthomosaic in a few steps.</p>"},{"location":"getting-started/quickstart/#sample-raster","title":"Sample raster","text":"<p>A small test raster is included in the repository at <code>assets/20240130_zf2tower_m3m_rgb_test_crop.tif</code>. You can use it to try the commands below without needing your own data.</p>"},{"location":"getting-started/quickstart/#using-a-preset-configuration","title":"Using a preset configuration","text":"<p>CanopyRS ships with preset pipelines. The fastest way to get started is to use one of them directly via <code>infer.py</code>.</p> <p>Single raster/orthomosaic input (<code>-i</code>):</p> <pre><code>python infer.py -c &lt;CONFIG_NAME&gt; -i &lt;PATH_TO_TIF&gt; -o &lt;PATH_TO_OUTPUT_FOLDER&gt;\n</code></pre> <p>Folder of already tiled geo-referenced images (<code>-t</code>):</p> <pre><code>python infer.py -c &lt;CONFIG_NAME&gt; -t &lt;PATH_TO_TILES_FOLDER&gt; -o &lt;PATH_TO_OUTPUT_FOLDER&gt;\n</code></pre>"},{"location":"getting-started/quickstart/#command-line-arguments","title":"Command-line arguments","text":"Argument Description <code>-c</code> Config name (folder name under <code>canopyrs/config/</code>, see Presets for a list of predefined configs.) <code>-i</code> Input path to a single raster/orthomosaic <code>-t</code> Input path to a folder of geo-referenced .tif tiles <code>-o</code> Output path"},{"location":"getting-started/quickstart/#understanding-the-output","title":"Understanding the output","text":"<p>The output folder will contain one subfolder per component that ran, containing output files such as:</p> <ul> <li>GeoPackage (<code>.gpkg</code>) \u2014 for example predicted tree polygons with scores</li> <li>COCO JSON \u2014 predictions in COCO format (used internally between components, can also be used to visualize per-tile predictions, see TODO)</li> </ul> <p>If the chosen pipeline configuration produced a GeoDataFrame containing polygon results, it will be present at the root of your output folder.</p>"},{"location":"getting-started/quickstart/#choosing-the-right-preset","title":"Choosing the right preset","text":"<p>See Presets for full details.</p>"},{"location":"user-guide/components/","title":"Components","text":"<p>Each component in the pipeline is responsible for a single stage of processing. All components share the same interface: they receive a <code>DataState</code>, do their work, and return a <code>ComponentResult</code>.</p> <p>All components can also be run individually \u2014 see Standalone Usage.</p>"},{"location":"user-guide/components/#tilerizer","title":"Tilerizer","text":"<p>Splits a large orthomosaic into smaller, overlapping tiles suitable for model inference.</p> <p>Tile types:</p> Type Description <code>tile</code> Unlabeled regular-grid tiles (input to detector or segmenter) <code>tile_labeled</code> Labeled tiles with COCO annotations (input to prompted segmenter) <code>polygon</code> Per-polygon tiles (input to classifier) <p>You can select the type of tilerizer you want using the <code>tile_type</code> parameter in your tilerizer config.</p> <p>Requires: <code>imagery_path</code> (+ <code>infer_gdf</code> for <code>tile_labeled</code> and <code>polygon</code>)</p> <p>Produces: <code>tiles_path</code>, optionally <code>infer_coco_path</code></p>"},{"location":"user-guide/components/#detector","title":"Detector","text":"<p>Runs object detection on image tiles, producing bounding box predictions with confidence scores.</p> <p>Requires: <code>tiles_path</code></p> <p>Produces: <code>infer_gdf</code> (geometry, object_id, tile_path, score, class), <code>infer_coco_path</code></p>"},{"location":"user-guide/components/#segmenter","title":"Segmenter","text":"<p>Refines bounding box detections into instance segmentation masks. Can operate in prompted mode (using detector boxes) or unprompted mode.</p> <p>Requires: <code>tiles_path</code>, optionally <code>infer_coco_path</code> (prompted mode)</p> <p>Produces: <code>infer_gdf</code> with updated mask geometries, <code>infer_coco_path</code></p>"},{"location":"user-guide/components/#aggregator","title":"Aggregator","text":"<p>Merges overlapping detections from tiled inference using non-maximum suppression (NMS). Produces the final per-tree polygons.</p> <p>Requires: <code>infer_gdf</code> with geometry, object_id, tile_path, and detector_score and/or segmenter_score columns</p> <p>Produces: aggregated <code>infer_gdf</code> with <code>aggregator_score</code></p>"},{"location":"user-guide/components/#classifier","title":"Classifier","text":"<p>Classifies each detected/segmented tree into categories (e.g. species).</p> <p>Requires: <code>tiles_path</code>, <code>infer_coco_path</code></p> <p>Produces: classification scores and predictions in <code>infer_gdf</code></p>"},{"location":"user-guide/components/#runtime-validation","title":"Runtime validation","text":"<p>Every component is decorated with <code>@validate_requirements</code>. At runtime, before the component logic executes, the decorator checks that all required state keys and GDF columns are present \u2014 and raises a clear error with hints if anything is missing.</p>"},{"location":"user-guide/configuration/","title":"Configuration","text":""},{"location":"user-guide/configuration/#configuration","title":"Configuration","text":"<p>CanopyRS pipelines are defined in YAML. A pipeline config lists the components to run and their parameters.</p>"},{"location":"user-guide/configuration/#pipeline-config-structure","title":"Pipeline config structure","text":"<pre><code>components_configs:\n  - tilerizer:\n      tile_type: tile\n      tile_size: 1777\n      tile_overlap: 0.75\n      ground_resolution: 0.045\n\n  - detector: detectors/dino_swinL_multi_NQOS.yaml\n\n  - aggregator:\n      nms_algorithm: 'iou'\n      score_threshold: 0.5\n      nms_threshold: 0.7\n      edge_band_buffer_percentage: 0.05\n      scores_weights: { 'detector_score': 1.0 }\n</code></pre> <p>Each entry is either:</p> <ul> <li>Inline config \u2014 parameters specified directly in the pipeline YAML</li> <li> <p>Reference \u2014 a path to a reusable component config, for example a detector in <code>canopyrs/config/detectors/</code> or a segmenter in <code>canopyrs/config/segmenters/</code>. See our Model Zoo for the full list.</p> <p>Example reusable detector config (<code>detectors/dino_swinL_multi_NQOS.yaml</code>):</p> </li> </ul> <pre><code>model: dino_detrex\narchitecture: dino/configs/dino-swin/dino_swin_large_384_5scale_36ep.py\ncheckpoint_path: 'https://huggingface.co/CanopyRS/dino-swin-l-384-multi-NQOS/resolve/main/model_best.pth'\nbatch_size: 1\nbox_predictions_per_image: 500\nnum_classes: 1\n</code></pre>"},{"location":"user-guide/configuration/#key-parameters","title":"Key parameters","text":""},{"location":"user-guide/configuration/#tilerizer","title":"Tilerizer","text":"Parameter Description <code>tile_type</code> <code>tile</code>, <code>tile_labeled</code>, or <code>polygon</code> <code>tile_size</code> Tile size in pixels <code>tile_overlap</code> Overlap ratio between tiles (0\u20131) <code>ground_resolution</code> Target ground resolution in meters/pixel (cannot be set together with <code>scale_factor</code>) <code>scale_factor</code> Scale factor to resize tiles (cannot be set together with <code>ground_resolution</code>). A value of 1.0 keeps the source raster resolution as-is."},{"location":"user-guide/configuration/#detector","title":"Detector","text":"Parameter Description <code>model</code> Model type identifier (e.g. <code>dino_detrex</code>, <code>faster_rcnn_detectron2</code>, <code>deepforest</code>) <code>architecture</code> Model architecture configuration path <code>checkpoint_path</code> Path or URL to pre-trained model weights <code>batch_size</code> Inference batch size <code>num_classes</code> Number of classes to detect (typically <code>1</code> for trees) <code>box_predictions_per_image</code> Maximum number of predictions per image <code>augmentation_image_size</code> Resize image size. A single int (e.g. <code>1024</code>) for a fixed size, or a <code>[min, max]</code> list (e.g. <code>[1024, 1777]</code>) where images smaller than min are upscaled and images larger than max are downscaled"},{"location":"user-guide/configuration/#segmenter","title":"Segmenter","text":"Parameter Description <code>model</code> Model type identifier (e.g. <code>sam2</code>, <code>sam3</code>, <code>detectree2</code>) <code>architecture</code> Model architecture variant (e.g. <code>l</code> for large) <code>checkpoint_path</code> Path or URL to pre-trained model weights <code>image_batch_size</code> Number of images processed at once (sometimes ignored by models like SAM that rely exclusively on <code>box_batch_size</code>) <code>box_batch_size</code> Number of prompt boxes processed at once per image <code>augmentation_image_size</code> Resize image size. A single int (e.g. <code>1024</code>) for a fixed size, or a <code>[min, max]</code> list (e.g. <code>[1024, 1777]</code>) where images smaller than min are upscaled and images larger than max are downscaled <code>pp_n_workers</code> Number of workers for parallel post-processing. Reduce this number if you have a limited amount of CPU cores. <code>pp_down_scale_masks_px</code> Downscale masks to this pixel size for more efficient post-processing <code>pp_simplify_tolerance</code> Polygon simplification tolerance <code>pp_remove_rings</code> Remove holes/rings from segmentation masks <code>pp_remove_small_geoms</code> Remove geometries smaller than this area threshold"},{"location":"user-guide/configuration/#aggregator","title":"Aggregator","text":"Parameter Description <code>nms_algorithm</code> <code>iou</code> (detection or segmentation) or <code>ioa-disambiguate</code> (segmentation only) <code>score_threshold</code> Minimum score to keep a detection <code>nms_threshold</code> IoU threshold for suppression <code>scores_weights</code> Weight per score column for combined scoring <code>scores_weighting_method</code> How to combine scores (e.g. <code>weighted_geometric_mean</code>)"},{"location":"user-guide/configuration/#classifier","title":"Classifier","text":"<p>To be added.</p>"},{"location":"user-guide/data/","title":"Data","text":"<p>In order to train or benchmark models, you will need data. In addition to SelvaBox, we provide several other pre-processed datasets.</p>"},{"location":"user-guide/data/#available-datasets","title":"Available datasets","text":"Dataset Size Link SelvaBox ~31.5 GB HuggingFace SelvaMask ~3.3 GB HuggingFace Detectree2 ~1.5 GB HuggingFace NeonTreeEvaluation ~3.3 GB HuggingFace OAM-TCD ~32.2 GB HuggingFace BCI50ha ~27.0 GB HuggingFace QuebecTrees ~6.0 GB HuggingFace"},{"location":"user-guide/data/#downloading-datasets","title":"Downloading datasets","text":"<p>To download and extract datasets automatically for use with our benchmark or training scripts, we provide a download tool.</p> <p>For example, to download SelvaBox and Detectree2 datasets:</p> Linux / macOSWindows (PowerShell) <pre><code>python -m canopyrs.tools.detection.download_datasets \\\n  -d SelvaBox Detectree2 \\\n  -o &lt;DATA_ROOT&gt;\n</code></pre> <pre><code>python -m canopyrs.tools.detection.download_datasets `\n  -d SelvaBox Detectree2 `\n  -o &lt;DATA_ROOT&gt;\n</code></pre> <p>After extraction, the datasets will be in COCO format (the same as geodataset's tilerizers output).</p>"},{"location":"user-guide/data/#data-structure","title":"Data structure","text":"<p>Your <code>&lt;DATA_ROOT&gt;</code> folder will contain one or more 'locations' folders, each containing individual 'rasters' folders. These contain .json COCO annotations and tiles for minimum one fold (train, valid, test...).</p> <p>For SelvaBox and Detectree2 datasets, the structure looks like this:</p> <pre><code>&lt;DATA_ROOT&gt;\n\u251c\u2500\u2500 brazil_zf2                         (-&gt; Brazil location of SelvaBox)\n\u2502   \u251c\u2500\u2500 20240130_zf2quad_m3m_rgb       (-&gt; one of the Brazil location rasters)\n\u2502   \u2502   \u251c\u2500\u2500 tiles/\n\u2502   \u2502   \u2502  \u251c\u2500\u2500 valid/\n\u2502   \u2502   \u2502  \u2502  \u251c\u2500\u2500 20240130_zf2quad_m3m_rgb_tile_valid_1777_gr0p045_0_6216.tif\n\u2502   \u2502   \u2502  \u2502  \u251c\u2500\u2500 ...\n\u2502   \u2502   \u251c\u2500\u2500  20240130_zf2quad_m3m_rgb_coco_gr0p045_valid.json\n\u2502   \u2502   \u2514\u2500\u2500  ...\n\u2502   \u251c\u2500\u2500 20240130_zf2tower_m3m_rgb\n\u2502   \u251c\u2500\u2500 20240130_zf2transectew_m3m_rgb\n\u2502   \u2514\u2500\u2500 20240131_zf2campirana_m3m_rgb\n\u251c\u2500\u2500 ecuador_tiputini                   (-&gt; Ecuador location of SelvaBox)\n\u2502   \u251c\u2500\u2500 ...\n\u251c\u2500\u2500 malaysia_detectree2                (-&gt; Malaysia location of Detectree2)\n\u2502   \u251c\u2500\u2500 ...\n\u2514\u2500\u2500 panama_aguasalud                   (-&gt; Panama location of SelvaBox)\n</code></pre> <p>Each additional dataset will add one or more locations folders.</p>"},{"location":"user-guide/data/#example-location-folders-by-dataset","title":"Example location folders by dataset","text":"<ul> <li>SelvaBox: <code>brazil_zf2</code>, <code>ecuador_tiputini</code>, <code>panama_aguasalud</code></li> <li>Detectree2: <code>malaysia_detectree2</code></li> </ul>"},{"location":"user-guide/evaluation/","title":"Evaluation","text":"<p>CanopyRS provides tools for finding optimal NMS parameters and benchmarking models on test datasets.</p>"},{"location":"user-guide/evaluation/#finding-optimal-nms-parameters","title":"Finding optimal NMS parameters","text":"<p>To find the optimal NMS parameters for your model (<code>nms_iou_threshold</code> and <code>nms_score_threshold</code>), use the <code>find_optimal_raster_nms.py</code> tool script. This script runs a grid search over NMS parameters and evaluates results using COCO evaluation metrics at a chosen IoU threshold.</p>"},{"location":"user-guide/evaluation/#iou-threshold-options","title":"IoU threshold options","text":"<ul> <li><code>--eval_iou_threshold 0.75</code> for RF1\u2087\u2085 (default)</li> <li><code>--eval_iou_threshold 50:95</code> for COCO-style sweep (RF1<sub>50:95</sub>)</li> <li>Comma-separated lists are also accepted: <code>--eval_iou_threshold 0.50,0.65,0.80</code></li> </ul>"},{"location":"user-guide/evaluation/#example-finding-nms-parameters-for-rf175","title":"Example: Finding NMS parameters for RF1\u2087\u2085","text":"<p>To find NMS parameters for the DINO Swin-L multi-NQOS detector on the validation set of SelvaBox and Detectree2:</p> Linux / macOSWindows (PowerShell) <pre><code>python -m canopyrs.tools.detection.find_optimal_raster_nms \\\n  -c canopyrs/config/detectors/dino_swinL_multi_NQOS.yaml \\\n  -d SelvaBox Detectree2 \\\n  -r &lt;DATA_ROOT&gt; \\\n  -o &lt;OUTPUT_PATH&gt; \\\n  --n_workers 6 \\\n  --eval_iou_threshold 0.75\n</code></pre> <pre><code>python -m canopyrs.tools.detection.find_optimal_raster_nms `\n  -c canopyrs/config/detectors/dino_swinL_multi_NQOS.yaml `\n  -d SelvaBox Detectree2 `\n  -r &lt;DATA_ROOT&gt; `\n  -o &lt;OUTPUT_PATH&gt; `\n  --n_workers 6 `\n  --eval_iou_threshold 0.75\n</code></pre>"},{"location":"user-guide/evaluation/#example-finding-nms-parameters-for-rf15095","title":"Example: Finding NMS parameters for RF1<sub>50:95</sub>","text":"Linux / macOSWindows (PowerShell) <pre><code>python -m canopyrs.tools.detection.find_optimal_raster_nms \\\n  -c canopyrs/config/detectors/dino_swinL_multi_NQOS.yaml \\\n  -d SelvaBox Detectree2 \\\n  -r &lt;DATA_ROOT&gt; \\\n  -o &lt;OUTPUT_PATH&gt; \\\n  --n_workers 6 \\\n  --eval_iou_threshold 50:95\n</code></pre> <pre><code>python -m canopyrs.tools.detection.find_optimal_raster_nms `\n  -c canopyrs/config/detectors/dino_swinL_multi_NQOS.yaml `\n  -d SelvaBox Detectree2 `\n  -r &lt;DATA_ROOT&gt; `\n  -o &lt;OUTPUT_PATH&gt; `\n  --n_workers 6 `\n  --eval_iou_threshold 50:95\n</code></pre>"},{"location":"user-guide/evaluation/#performance-notes","title":"Performance notes","text":"<p>Depending on how many rasters there are in the datasets you select, the search could take from a few tens of minutes to a few hours. If you have lots of CPU cores, we recommend increasing the number of workers.</p> <p>For more information on parameters:</p> <pre><code>python -m canopyrs.tools.detection.find_optimal_raster_nms --help\n</code></pre>"},{"location":"user-guide/evaluation/#benchmarking","title":"Benchmarking","text":"<p>To benchmark a model on test or valid sets of datasets, use the <code>benchmark.py</code> tool script.</p> <p>This script runs the model and evaluates results using tile-level COCO metrics (mAP and mAR).</p>"},{"location":"user-guide/evaluation/#raster-level-evaluation-rf1","title":"Raster-level evaluation (RF1)","text":"<p>To run raster-level evaluation (RF1) in addition to tile-level, you must pass values for <code>--nms_threshold</code> and <code>--score_threshold</code>. To find these parameter values, run <code>find_optimal_raster_nms.py</code> on the validation set of one (or more) datasets, as described above.</p> <p>The benchmark will then run a single raster-level aggregation with those values and report RF1 at the chosen IoU setting.</p>"},{"location":"user-guide/evaluation/#important-use-consistent-iou-thresholds","title":"Important: Use consistent IoU thresholds","text":"<p>Always use the same <code>--eval_iou_threshold</code> value when finding NMS parameters and when running the final benchmark. If you optimize NMS for RF1\u2087\u2085 but benchmark with RF1<sub>50:95</sub>, your NMS parameters will not be optimal for that metric.</p>"},{"location":"user-guide/evaluation/#example-benchmarking-with-rf15095","title":"Example: Benchmarking with RF1<sub>50:95</sub>","text":"<p>To benchmark the DINO Swin-L multi-NQOS detector on the test set of SelvaBox and Detectree2:</p> Linux / macOSWindows (PowerShell) <pre><code>python -m canopyrs.tools.detection.benchmark \\\n  -c canopyrs/config/detectors/dino_swinL_multi_NQOS.yaml \\\n  -d SelvaBox Detectree2 \\\n  -r &lt;DATA_ROOT&gt; \\\n  -o &lt;OUTPUT_PATH&gt; \\\n  --nms_threshold 0.7 \\\n  --score_threshold 0.5 \\\n  --eval_iou_threshold 50:95\n</code></pre> <pre><code>python -m canopyrs.tools.detection.benchmark `\n  -c canopyrs/config/detectors/dino_swinL_multi_NQOS.yaml `\n  -d SelvaBox Detectree2 `\n  -r &lt;DATA_ROOT&gt; `\n  -o &lt;OUTPUT_PATH&gt; `\n  --nms_threshold 0.7 `\n  --score_threshold 0.5 `\n  --eval_iou_threshold 50:95\n</code></pre> <p>By default, evaluation is done on the test set.</p> <p>For more information on parameters:</p> <pre><code>python -m canopyrs.tools.detection.benchmark --help\n</code></pre>"},{"location":"user-guide/evaluation/#programmatic-usage","title":"Programmatic usage","text":"<p>You can also use the benchmarker classes directly in Python for more control.</p>"},{"location":"user-guide/evaluation/#detector-example","title":"Detector example","text":"<pre><code>from canopyrs.engine.benchmark import DetectorBenchmarker\nfrom canopyrs.engine.config_parsers import DetectorConfig, AggregatorConfig\n\ndetector_config = DetectorConfig.from_yaml(\"canopyrs/config/detectors/dino_swinL_multi_NQOS.yaml\")\n\n# COCO-style IoU sweep for RF1_50:95\neval_ious = [0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]\n\n# Detector-only: all confidence comes from the detector\naggregator_base = AggregatorConfig(\n    nms_algorithm=\"iou\",\n    detector_score_weight=1.0,\n    segmenter_score_weight=0.0,\n)\n\n# Step 1: Find optimal NMS parameters on validation set\nvalid_benchmarker = DetectorBenchmarker(\n    output_folder=\"./output/benchmark/valid\",\n    fold_name=\"valid\",\n    raw_data_root=\"/data/canopyrs\",\n    eval_iou_threshold=eval_ious,\n)\n\nbest_aggregator = valid_benchmarker.find_optimal_nms_iou_threshold(\n    detector_config=detector_config,\n    base_aggregator_config=aggregator_base,\n    dataset_names=[\"SelvaBox\", \"Detectree2\"],\n    nms_iou_thresholds=[i / 20 for i in range(1, 21)],# these parameters define over which values the grid search should be ran\n    nms_score_thresholds=[i / 20 for i in range(1, 21)],\n    n_workers=6,\n)\n\n# Step 2: Benchmark on test set using optimal NMS parameters\ntest_benchmarker = DetectorBenchmarker(\n    output_folder=\"./output/benchmark/test\",\n    fold_name=\"test\",\n    raw_data_root=\"/data/canopyrs\",\n    eval_iou_threshold=eval_ious,\n)\n\ntile_metrics, raster_metrics = test_benchmarker.benchmark(\n    detector_config=detector_config,\n    aggregator_config=best_aggregator,\n    dataset_names=[\"SelvaBox\", \"Detectree2\"],\n)\n</code></pre>"},{"location":"user-guide/evaluation/#segmenter-example-end-to-end","title":"Segmenter example (end-to-end)","text":"<pre><code>from canopyrs.engine.benchmark import SegmenterBenchmarker\nfrom canopyrs.engine.config_parsers import SegmenterConfig, AggregatorConfig\n\nsegmenter_config = SegmenterConfig.from_yaml(\"canopyrs/config/segmenters/mask2former_swinL_multi_selvamask.yaml\")\n\n# COCO-style IoU sweep for RF1_50:95\neval_ious = [0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]\n\n# End-to-end segmenter: all confidence comes from the segmenter\naggregator_base = AggregatorConfig(\n    nms_algorithm=\"iou\",\n    detector_score_weight=0.0,\n    segmenter_score_weight=1.0,\n)\n\n# Step 1: Find optimal NMS parameters on validation set\nvalid_benchmarker = SegmenterBenchmarker(\n    output_folder=\"./output/benchmark/valid\",\n    fold_name=\"valid\",\n    raw_data_root=\"/data/canopyrs\",\n    eval_iou_threshold=eval_ious,\n)\n\nbest_aggregator = valid_benchmarker.find_optimal_nms_iou_threshold(\n    segmenter_config=segmenter_config,\n    base_aggregator_config=aggregator_base,\n    dataset_names=[\"SelvaMask\"],\n    nms_iou_thresholds=[i / 20 for i in range(1, 21)],\n    nms_score_thresholds=[i / 20 for i in range(1, 21)],\n    n_workers=6,\n)\n\n# Step 2: Benchmark on test set using optimal NMS parameters\ntest_benchmarker = SegmenterBenchmarker(\n    output_folder=\"./output/benchmark/test\",\n    fold_name=\"test\",\n    raw_data_root=\"/data/canopyrs\",\n    eval_iou_threshold=eval_ious,\n)\n\ntile_metrics, raster_metrics = test_benchmarker.benchmark(\n    segmenter_config=segmenter_config,\n    aggregator_config=best_aggregator,\n    dataset_names=[\"SelvaMask\"],\n)\n</code></pre>"},{"location":"user-guide/evaluation/#segmenter-example-detector-prompted-sam3","title":"Segmenter example (detector + prompted SAM3)","text":"<pre><code>from canopyrs.engine.benchmark import SegmenterBenchmarker\nfrom canopyrs.engine.config_parsers import DetectorConfig, SegmenterConfig, AggregatorConfig\n\ndetector_config = DetectorConfig.from_yaml(\"canopyrs/config/detectors/dino_swinL_multi_NQOS_selvamask_FT.yaml\")\nsegmenter_config = SegmenterConfig.from_yaml(\"canopyrs/config/segmenters/sam3_multi_selvamask_FT.yaml\")\n\n# COCO-style IoU sweep for RF1_50:95\neval_ious = [0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]\n\n# Prompted SAM3: final score is a weighted blend of detector and segmenter confidence.\n# Equal weights (0.5/0.5) is a reasonable default; rebalance if one model is more reliable.\naggregator_base = AggregatorConfig(\n    nms_algorithm=\"ioa-disambiguate\",\n    detector_score_weight=0.5,\n    segmenter_score_weight=0.5,\n)\n\n# Step 1: Find optimal NMS parameters on validation set\nvalid_benchmarker = SegmenterBenchmarker(\n    output_folder=\"./output/benchmark/valid\",\n    fold_name=\"valid\",\n    raw_data_root=\"/data/canopyrs\",\n    eval_iou_threshold=eval_ious,\n)\n\nbest_aggregator = valid_benchmarker.find_optimal_nms_iou_threshold(\n    segmenter_config=segmenter_config,\n    prompter_detector_config=detector_config,\n    base_aggregator_config=aggregator_base,\n    dataset_names=[\"SelvaMask\"],\n    nms_iou_thresholds=[i / 20 for i in range(1, 21)],      # these parameters define over which values the grid search should be ran\n    nms_score_thresholds=[i / 20 for i in range(1, 21)],\n    n_workers=6,\n)\n\n# Step 2: Benchmark on test set\ntest_benchmarker = SegmenterBenchmarker(\n    output_folder=\"./output/benchmark/test\",\n    fold_name=\"test\",\n    raw_data_root=\"/data/canopyrs\",\n    eval_iou_threshold=eval_ious,\n)\n\ntile_metrics, raster_metrics = test_benchmarker.benchmark(\n    segmenter_config=segmenter_config,\n    prompter_detector_config=detector_config,\n    aggregator_config=best_aggregator,\n    dataset_names=[\"SelvaMask\"],\n)\n</code></pre>"},{"location":"user-guide/evaluation/#aggregating-results-across-seeds","title":"Aggregating results across seeds","text":"<p>If you ran multiple training seeds, you can compute mean/std across runs for both tile-level and raster-level metrics, then merge them into a single table:</p> <pre><code># Assuming you collected per-seed results:\n# tile_metrics_list = [tile_metrics_seed1, tile_metrics_seed2, tile_metrics_seed3]\n# raster_metrics_list = [raster_metrics_seed1, raster_metrics_seed2, raster_metrics_seed3]\n\nsummary_tile = DetectorBenchmarker.compute_mean_std_metric_tables(\n    tile_metrics_list,\n    output_csv=\"./output/benchmark/tile_summary.csv\",\n)\n\nsummary_raster = DetectorBenchmarker.compute_mean_std_metric_tables(\n    raster_metrics_list,\n    output_csv=\"./output/benchmark/raster_summary.csv\",\n)\n\n# Merge tile and raster summaries into one table\ncombined = DetectorBenchmarker.merge_tile_and_raster_summaries(\n    summary_tile,\n    summary_raster,\n    output_csv=\"./output/benchmark/combined_summary.csv\",\n    tile_prefix=\"tile\",\n    raster_prefix=\"raster\",\n)\n</code></pre>"},{"location":"user-guide/model-zoo/","title":"Model Zoo","text":"<p>This page lists every model supported by CanopyRS, grouped by pipeline stage. If you just want a ready-made configuration, see Presets. If you want to build a custom pipeline, use the tables below to pick models for each stage and reference them in your <code>pipeline.yaml</code>.</p> <p>All checkpoints are downloaded automatically the first time a model is used.</p>"},{"location":"user-guide/model-zoo/#detectors","title":"Detectors","text":"<p>Detectors produce bounding boxes around individual trees.</p> Config file Architecture Training data Checkpoint Config <code>detectors/dino_swinL_multi_NQOS.yaml</code> DINO\u00a0+\u00a0Swin\u2011L\u00a0384 Multi-resolution, multi-dataset (NeonTrees, QuebecTrees, OAM-TCD, SelvaBox) HuggingFace (CanopyRS) YAML <code>detectors/dino_swinL_multi_NQOS_selvamask_FT.yaml</code> DINO\u00a0+\u00a0Swin\u2011L\u00a0384 Multi-resolution, multi-dataset (NeonTrees, QuebecTrees, OAM-TCD, SelvaBox), fine-tuned on SelvaMask HuggingFace (CanopyRS) YAML <code>detectors/dino_r50_single_S.yaml</code> DINO\u00a0+\u00a0ResNet\u201150 SelvaBox (single resolution, 6 cm/px) HuggingFace (CanopyRS) YAML <code>detectors/fasterrcnn_r50_single_S.yaml</code> Faster\u00a0R\u2011CNN\u00a0+\u00a0ResNet\u201150 SelvaBox (single resolution, 10 cm/px) HuggingFace (CanopyRS) YAML"},{"location":"user-guide/model-zoo/#external-models","title":"External models","text":"<p>These detectors come from third-party projects. CanopyRS wraps them so they can be used as pipeline components.</p> Config file Architecture Project Training data Checkpoint Config <code>detectors/deepforest.yaml</code> RetinaNet\u00a0+\u00a0ResNet\u201150 DeepForest NeonTrees DeepForest YAML"},{"location":"user-guide/model-zoo/#segmenters","title":"Segmenters","text":"<p>Segmenters produce per-tree instance masks. They can be prompted (fed bounding boxes from a detector) or unprompted (run directly on tiles).</p>"},{"location":"user-guide/model-zoo/#prompted-segmenters","title":"Prompted segmenters","text":"<p>These models take bounding boxes as input and produce a mask for each box. Chain them after a detector.</p> Config file Architecture Training data Checkpoint Config <code>segmenters/sam2_L.yaml</code> SAM\u00a02\u00a0Large SA-1B (foundation model) Meta YAML <code>segmenters/sam3_multi_selvamask_FT.yaml</code> SAM\u00a03 SA-1B, fine-tuned on SelvaMask HuggingFace (CanopyRS) YAML <p>Note: SAM 3 requires a Hugging Face access request from Meta before first use. See Installation \u2014 SAM 3 access request for details.</p>"},{"location":"user-guide/model-zoo/#unprompted-segmenters","title":"Unprompted segmenters","text":"<p>These models perform end-to-end instance segmentation without requiring bounding box prompts.</p> Config file Architecture Training data Checkpoint Config <code>segmenters/mask2former_swinL_multi_selvamask.yaml</code> Mask2Former\u00a0+\u00a0Swin\u2011L SelvaMask HuggingFace (CanopyRS) YAML <code>segmenters/maskrcnn_r50_multi_selvamask.yaml</code> Mask\u00a0R\u2011CNN\u00a0+\u00a0ResNet\u201150 SelvaMask HuggingFace (CanopyRS) YAML"},{"location":"user-guide/model-zoo/#external-models_1","title":"External models","text":"<p>These segmenters come from third-party projects. CanopyRS wraps them so they can be used as pipeline components.</p> Config file Architecture Project Training data Checkpoint Config <code>segmenters/detectree2_flexi.yaml</code> Mask\u00a0R\u2011CNN\u00a0+\u00a0ResNet\u2011101 Detectree2 Detectree2 + urban data Zenodo YAML <code>segmenters/detectree2_randresizefull.yaml</code> Mask\u00a0R\u2011CNN\u00a0+\u00a0ResNet\u2011101 Detectree2 Detectree2 Zenodo YAML"},{"location":"user-guide/model-zoo/#using-a-model-in-a-custom-pipeline","title":"Using a model in a custom pipeline","text":"<p>To use any model listed above, reference its config in your <code>pipeline.yaml</code>:</p> <p>Detector:</p> <pre><code>- detector: detectors/dino_swinL_multi_NQOS.yaml\n</code></pre> <p>Segmenter:</p> <pre><code>- segmenter: segmenters/sam2_L.yaml\n</code></pre> <p>See Configuration for the full list of parameters you can override.</p>"},{"location":"user-guide/pipeline/","title":"Pipeline","text":"<p>The <code>Pipeline</code> class is the orchestrator. It sequences components, manages state, handles file I/O, and coordinates background tasks.</p>"},{"location":"user-guide/pipeline/#lifecycle","title":"Lifecycle","text":"<pre><code>Pipeline.__init__()\n    \u251c\u2500\u2500 Assign component IDs and output paths\n    \u251c\u2500\u2500 Print flow chart (visualizes state/column flow)\n    \u2514\u2500\u2500 Validate pipeline (catch missing requirements early)\n\nPipeline.run()\n    \u2514\u2500\u2500 For each component:\n        \u251c\u2500\u2500 Wait for any required background tasks (e.g. COCO generation)\n        \u251c\u2500\u2500 Run component.__call__(data_state)\n        \u2514\u2500\u2500 Process result (merge GDF, save files, queue async tasks)\n</code></pre>"},{"location":"user-guide/pipeline/#flow-chart","title":"Flow chart","text":"<p>When a pipeline is constructed, it automatically prints a colored flow chart showing which state keys and GDF columns are available, produced, required, or missing at each stage. This makes it straightforward to spot configuration errors before any inference runs.</p>"},{"location":"user-guide/pipeline/#state-management","title":"State management","text":"<p>All shared state lives in a single <code>DataState</code> object. Components read from it and return a <code>ComponentResult</code>. The pipeline applies the result to the state \u2014 components never mutate <code>DataState</code> directly.</p> <p>Key state keys:</p> Key Description <code>imagery_path</code> Path to the input orthomosaic <code>tiles_path</code> Path to the generated tiles directory <code>infer_gdf</code> The working GeoDataFrame of detections <code>infer_coco_path</code> Path to the current COCO annotation file <code>product_name</code> Derived from the input filename, used for output naming"},{"location":"user-guide/pipeline/#background-tasks","title":"Background tasks","text":"<p>COCO file generation is expensive and is queued as a background process after each component that produces one. The pipeline automatically waits for the relevant COCO file to finish before running any component that requires it \u2014 no manual coordination needed.</p>"},{"location":"user-guide/pipeline/#standalone-component-usage","title":"Standalone component usage","text":"<p>Each component can also be run individually outside a pipeline. See the Standalone Usage page for details and examples.</p>"},{"location":"user-guide/presets/","title":"Presets","text":"<p>CanopyRS ships with preset pipeline configurations for common use cases. Each preset is a YAML file under <code>canopyrs/config/pipelines/</code>.</p>"},{"location":"user-guide/presets/#available-presets","title":"Available presets","text":"<p>We provide different preset configs depending on your GPU resources and use case. You can find these config files in <code>canopyrs/config/pipelines/</code>\u2014feel free to copy and adapt them to optimize inference on your data.</p>"},{"location":"user-guide/presets/#instance-segmentation-sam-3","title":"Instance Segmentation (SAM\u00a03)","text":"<p>Note: SAM 3 requires a Hugging Face access request from Meta before first use. See Installation \u2014 SAM 3 access request for details.</p> Config file Architecture Train dataset(s) Performance/Requirements Quality Description <code>preset_seg_multi_NQOS_selvamask_SAM3_FT_quality.yaml</code> DINO\u00a0+\u00a0Swin\u2011L\u00a0384\u00a0+\u00a0SAM\u00a03 Multi-resolution, multi-dataset (NeonTrees, QuebecTrees, OAM-TCD, SelvaBox), fine-tuned on SelvaMask ~10 GB GPU memory Best (Recommended for best quality) SelvaMask fine-tuned DINO detector and SAM 3 segmenter at high resolution (4.5 cm/px GSD) for best quality. <code>preset_seg_multi_NQOS_selvamask_SAM3_FT_fast.yaml</code> DINO\u00a0+\u00a0Swin\u2011L\u00a0384\u00a0+\u00a0SAM\u00a03 Multi-resolution, multi-dataset (NeonTrees, QuebecTrees, OAM-TCD, SelvaBox), fine-tuned on SelvaMask ~10 GB GPU memory High (Recommended for a bit faster, high-quality inference) SelvaMask fine-tuned DINO detector and SAM 3 segmenter at lower resolution (7 cm/px GSD) and reduced tile overlap for faster inference."},{"location":"user-guide/presets/#instance-segmentation-sam-2","title":"Instance Segmentation (SAM\u00a02)","text":"Config file Architecture Train dataset(s) Performance/Requirements Quality Description <code>preset_seg_multi_NQOS_SAM2.yaml</code> DINO\u00a0+\u00a0Swin\u2011L\u00a0384\u00a0+\u00a0SAM\u00a02 Multi-resolution, multi-dataset (NeonTrees, QuebecTrees, OAM-TCD, SelvaBox) ~10 GB GPU memory High Same detector as <code>preset_det_multi_NQOS_dino_swinL.yaml</code>, with SAM 2 chained after detection for instance segmentations. <code>preset_seg_multi_NQOS_SAM2_smalltrees.yaml</code> DINO\u00a0+\u00a0Swin\u2011L\u00a0384\u00a0+\u00a0SAM\u00a02 Multi-resolution, multi-dataset (NeonTrees, QuebecTrees, OAM-TCD, SelvaBox) ~10 GB GPU memory High (Recommended for small trees) Optimized for smaller trees (up to ~15 m) with 4 cm/px GSD and smaller tiles. <code>preset_seg_multi_NQOS_SAM2_largetrees.yaml</code> DINO\u00a0+\u00a0Swin\u2011L\u00a0384\u00a0+\u00a0SAM\u00a02 Multi-resolution, multi-dataset (NeonTrees, QuebecTrees, OAM-TCD, SelvaBox) ~10 GB GPU memory High (Recommended for large trees) Optimized for larger trees (up to ~60 m) with 7 cm/px GSD and larger tiles."},{"location":"user-guide/presets/#instance-detection-only","title":"Instance Detection only","text":"Config file Architecture Train dataset(s) Performance/Requirements Quality Description <code>preset_det_multi_NQOS_dino_swinL.yaml</code> DINO\u00a0+\u00a0Swin\u2011L\u00a0384 Multi-resolution, multi-dataset (NeonTrees, QuebecTrees, OAM-TCD, SelvaBox) ~10 GB GPU memory High The best detection model from our paper. NMS hyper-parameters found using the RF1\u2087\u2085 metric. <code>preset_det_single_S_dino_r50.yaml</code> DINO\u00a0+\u00a0ResNet\u201150 SelvaBox (single resolution, 6 cm/px) Faster, lower memory Medium Single resolution model with lower memory footprint compared to Swin L-384 backbones. <code>preset_det_single_S_fasterrcnn_r50.yaml</code> Faster\u00a0R\u2011CNN\u00a0+\u00a0ResNet\u201150 SelvaBox (single resolution, 10 cm/px) Fastest, lowest memory Low Faster and lower memory footprint but lower quality."},{"location":"user-guide/presets/#how-to-use-a-preset","title":"How to use a preset","text":"<p>Pass the preset file name to <code>infer.py</code> with the <code>-c</code> flag:</p> <pre><code>python infer.py -c preset_seg_multi_NQOS_SAM2_smalltrees.yaml -i &lt;PATH_TO_TIF&gt; -o &lt;PATH_TO_OUTPUT_FOLDER&gt;\n</code></pre>"},{"location":"user-guide/presets/#customizing-a-preset","title":"Customizing a preset","text":"<p>Copy a preset YAML file, edit it, and point <code>-c</code> to your copy. All inline parameters can be tweaked without changing any code. See Configuration for further details on how to build your own pipeline.</p>"},{"location":"user-guide/standalone/","title":"Standalone Usage","text":"<p>Every component can be run individually outside a pipeline via its <code>run_standalone()</code> classmethod. Each method has a component-specific signature with explicit parameters, so you get clear parameter names and IDE autocomplete.</p> <p>For details on available config parameters (tile size, NMS thresholds, score weights, etc.), see Configuration.</p>"},{"location":"user-guide/standalone/#tilerizer","title":"Tilerizer","text":"<pre><code>from canopyrs.engine.components.tilerizer import TilerizerComponent\nfrom canopyrs.engine.config_parsers import TilerizerConfig\n\nresult = TilerizerComponent.run_standalone(\n    config=TilerizerConfig(tile_type='tile', tile_size=512, ...),\n    imagery_path='./raster.tif',\n    output_path='./output',\n)\nprint(result.tiles_path)\n</code></pre>"},{"location":"user-guide/standalone/#detector","title":"Detector","text":"<pre><code>from canopyrs.engine.components.detector import DetectorComponent\nfrom canopyrs.engine.config_parsers import DetectorConfig\n\nresult = DetectorComponent.run_standalone(\n    config=DetectorConfig(model='dino_detrex', ...),\n    tiles_path='./tiles',\n    output_path='./output',\n)\nprint(result.infer_gdf)\n</code></pre> <p>Using a config from a preset (see Model Zoo for available models):</p> <pre><code>config = DetectorConfig.from_yaml('canopyrs/config/detectors/dino_swinL_multi_NQOS.yaml')\n\nresult = DetectorComponent.run_standalone(\n    config=config,\n    tiles_path='./tiles',\n    output_path='./output',\n)\n</code></pre>"},{"location":"user-guide/standalone/#segmenter","title":"Segmenter","text":"<pre><code>from canopyrs.engine.components.segmenter import SegmenterComponent\nfrom canopyrs.engine.config_parsers import SegmenterConfig\n\nresult = SegmenterComponent.run_standalone(\n    config=SegmenterConfig(model='sam3', ...),\n    tiles_path='./tiles',\n    output_path='./output',\n    infer_coco_path='./coco.json',  # only if model requires box prompts\n)\nprint(result.infer_gdf)\n</code></pre> <p>Using a config from a preset (see Model Zoo for available models):</p> <pre><code>config = SegmenterConfig.from_yaml('canopyrs/config/segmenters/sam3_multi_selvamask_FT.yaml')\n\nresult = SegmenterComponent.run_standalone(\n    config=config,\n    tiles_path='./tiles',\n    output_path='./output',\n    infer_coco_path='./coco.json',\n)\n</code></pre>"},{"location":"user-guide/standalone/#aggregator","title":"Aggregator","text":"<pre><code>from canopyrs.engine.components.aggregator import AggregatorComponent\nfrom canopyrs.engine.config_parsers import AggregatorConfig\n\nresult = AggregatorComponent.run_standalone(\n    config=AggregatorConfig(nms_threshold=0.5, ...),\n    infer_gdf=my_detections_gdf,\n    output_path='./output',\n)\nprint(result.infer_gdf)\n</code></pre>"},{"location":"user-guide/standalone/#classifier","title":"Classifier","text":"<pre><code>from canopyrs.engine.components.classifier import ClassifierComponent\nfrom canopyrs.engine.config_parsers import ClassifierConfig\n\nresult = ClassifierComponent.run_standalone(\n    config=ClassifierConfig(model='resnet50', ...),\n    tiles_path='./polygon_tiles',\n    infer_coco_path='./coco.json',\n    output_path='./output',\n)\nprint(result.infer_gdf)\n</code></pre>"},{"location":"user-guide/standalone/#how-it-works","title":"How it works","text":"<p>Under the hood, <code>run_standalone()</code> delegates to the generic <code>run_component()</code> helper, which wraps the component in a single-component pipeline. Inputs are validated before execution \u2014 if something is missing, you get a clear error message listing what's needed.</p> <p>If you need more control, you can use <code>run_component()</code> directly:</p> <pre><code>from canopyrs.engine.pipeline import run_component\nfrom canopyrs.engine.components.detector import DetectorComponent\n\nresult = run_component(\n    component=DetectorComponent(config),\n    output_path='./output',\n    tiles_path='./tiles',\n)\n</code></pre>"},{"location":"user-guide/training/","title":"Training","text":"<p>We provide a <code>train.py</code> script to train detector and segmenter models on preprocessed datasets. You must download the datasets first (see Data).</p>"},{"location":"user-guide/training/#prerequisites","title":"Prerequisites","text":"<p>Our training pipeline requires wandb to be installed and configured for logging purposes.</p>"},{"location":"user-guide/training/#detector-training","title":"Detector training","text":"<p>To train a detector, copy and modify one of the config files under <code>canopyrs/config/detectors/</code>. For example, start from <code>dino_swinL_multi_NQOS.yaml</code>.</p>"},{"location":"user-guide/training/#workflow","title":"Workflow","text":"<ol> <li> <p>Download datasets:    <pre><code>python -m canopyrs.tools.detection.download_datasets -d SelvaBox Detectree2 -o /data\n</code></pre></p> </li> <li> <p>Copy and modify a detector config:    <pre><code>cp canopyrs/config/detectors/dino_swinL_multi_NQOS.yaml canopyrs/config/detectors/my_detector.yaml\n</code></pre>    Edit <code>my_detector.yaml</code> and update the training-specific fields marked with <code>TODO</code>:</p> <ul> <li><code>data_root_path</code> \u2014 path to your dataset root folder</li> <li><code>train_output_path</code> \u2014 path for model checkpoints and logs</li> <li><code>train_dataset_names</code> / <code>valid_dataset_names</code> \u2014 location folders to use. See Data for more info on data structure</li> <li><code>wandb_project</code> \u2014 your wandb project name</li> </ul> </li> <li> <p>Run training:</p> </li> </ol> Linux / macOSWindows (PowerShell) <pre><code>python train.py \\\n  -m detector \\\n  -c canopyrs/config/detectors/my_detector.yaml\n</code></pre> <pre><code>python train.py `\n  -m detector `\n  -c canopyrs/config/detectors/my_detector.yaml\n</code></pre>"},{"location":"user-guide/training/#configuration-reference","title":"Configuration reference","text":""},{"location":"user-guide/training/#model-parameters","title":"Model parameters","text":"Parameter Description <code>model</code> Model type: <code>dino_detrex</code> for detrex-based DINO models or <code>faster_rcnn_detectron2</code> for detectron2-based Faster R-CNN models <code>architecture</code> Model architecture (see supported architectures below) <code>checkpoint_path</code> Path to pretrained model checkpoint. Keep our pretrained checkpoint to fine-tune, or replace with a detrex COCO checkpoint. If left as <code>null</code> for a detectron2 model (Faster R-CNN), it will download a pretrained COCO checkpoint automatically."},{"location":"user-guide/training/#supported-architectures","title":"Supported architectures","text":"Model type Architecture DINO (Swin-L) <code>dino-swin/dino_swin_large_384_5scale_36ep.py</code> DINO (ResNet-50) <code>dino-resnet/dino_r50_4scale_24ep.py</code> Faster R-CNN <code>COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml</code>"},{"location":"user-guide/training/#data-parameters","title":"Data parameters","text":"Parameter Description <code>data_root_path</code> Path to your dataset root folder (the <code>&lt;DATA_ROOT&gt;</code> folder where extracted datasets are located) <code>train_dataset_names</code> List of location folder names to train on <code>valid_dataset_names</code> List of location folder names to validate on <code>train_output_path</code> Path to output folder for model checkpoints and logs <code>wandb_project</code> Name of the wandb project to log to"},{"location":"user-guide/training/#dataset-locations","title":"Dataset locations","text":"<p>SelvaBox has three locations: - <code>brazil_zf2</code> - <code>ecuador_tiputini</code> - <code>panama_aguasalud</code></p> <p>Detectree2 has one location: - <code>malaysia_detectree2</code></p> <p>You can choose to train on all locations or a subset of them.</p>"},{"location":"user-guide/training/#other-parameters","title":"Other parameters","text":"<p>You can also modify parameters such as <code>batch_size</code>, <code>lr</code>, and more in the config file.</p>"},{"location":"user-guide/training/#segmenter-training","title":"Segmenter training","text":"<p>To train a segmenter, copy and modify one of the config files under <code>canopyrs/config/segmenters/</code>. For example, start from <code>mask2former_swinL_multi_selvamask.yaml</code>.</p>"},{"location":"user-guide/training/#workflow_1","title":"Workflow","text":"<ol> <li> <p>Download datasets:    <pre><code>python -m canopyrs.tools.detection.download_datasets -d SelvaMask -o /data\n</code></pre></p> </li> <li> <p>Copy and modify a segmenter config:    <pre><code>cp canopyrs/config/segmenters/mask2former_swinL_multi_selvamask.yaml canopyrs/config/segmenters/my_segmenter.yaml\n</code></pre>    Edit <code>my_segmenter.yaml</code> and update the training-specific fields marked with <code>TODO</code>:</p> <ul> <li><code>data_root_path</code> \u2014 path to your dataset root folder</li> <li><code>train_output_path</code> \u2014 path for model checkpoints and logs</li> <li><code>train_dataset_names</code> / <code>valid_dataset_names</code> \u2014 location folders to use. See Data for more info on data structure</li> <li><code>wandb_project</code> \u2014 your wandb project name</li> </ul> </li> <li> <p>Run training:</p> </li> </ol> Linux / macOSWindows (PowerShell) <pre><code>python train.py \\\n  -m segmenter \\\n  -c canopyrs/config/segmenters/my_segmenter.yaml\n</code></pre> <pre><code>python train.py `\n  -m segmenter `\n  -c canopyrs/config/segmenters/my_segmenter.yaml\n</code></pre>"},{"location":"user-guide/training/#configuration-reference_1","title":"Configuration reference","text":""},{"location":"user-guide/training/#model-parameters_1","title":"Model parameters","text":"Parameter Description <code>model</code> Model type: <code>mask2former_detrex</code> for Mask2Former, <code>mask_rcnn_detectron2</code> for Mask R-CNN, or <code>sam3</code> for SAM 3 <code>architecture</code> Model architecture (see supported architectures below) <code>checkpoint_path</code> Path to pretrained model checkpoint. Keep our pretrained checkpoint to fine-tune, or replace with your own or a detrex/sam3 pretrained checkpoint. If left as <code>null</code> for a detectron2 model (Mask R-CNN), it will download a pretrained COCO checkpoint automatically."},{"location":"user-guide/training/#supported-architectures_1","title":"Supported architectures","text":"Model type Architecture Mask2Former (Swin-L) <code>mask2former/configs/maskformer2_swin_large_IN21k_384_bs16_100ep.py</code> Mask R-CNN (ResNet-50) <code>COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml</code> SAM 3 <code>l</code>"},{"location":"user-guide/training/#data-parameters_1","title":"Data parameters","text":"Parameter Description <code>data_root_path</code> Path to your dataset root folder (the <code>&lt;DATA_ROOT&gt;</code> folder where extracted datasets are located) <code>train_dataset_names</code> List of location folder names to train on <code>valid_dataset_names</code> List of location folder names to validate on <code>train_output_path</code> Path to output folder for model checkpoints and logs <code>wandb_project</code> Name of the wandb project to log to"},{"location":"user-guide/training/#other-parameters_1","title":"Other parameters","text":"<p>You can also modify parameters such as <code>batch_size</code>, <code>lr</code>, and more in the config file.</p>"}]}